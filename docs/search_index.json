[
["index.html", "Book’s title Book’s subtitle Chapter 1 Introduction", " Book’s title Book’s subtitle J. Renato Leripio 2022-02-01 Chapter 1 Introduction We will start this book by proposing some short yet very comprehensive exercises on the basics of data wrangling. However, this is by no means an easy or simple task. Since we spend a large portion of our time just cleaning and tidying up the datasets, doing the basics as readable and as efficient as possible is of great value. "],
["tidyverse.html", "Chapter 2 The basics of Tidyverse 2.1 What’s tidyverse? 2.2 Import 2.3 Wrangle", " Chapter 2 The basics of Tidyverse The purpose of this chapter is to introduce the main functions of the core packages in tidyverse. It’s not intended to be a thorough description of every function, rather the idea is to provide the basic tools so that those who are either base R users or aren’t R users at all can follow along with the rest of the book. For a more detailed approach to tidyverse, the best source is R for Data Science by Hadley Wickham &amp; Garrett Grolemund. For those who feel comfortable with tidyverse, a quick skim through the chapter might be enough. 2.1 What’s tidyverse? To start, what’s tidyverse? According to the official website: The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. From my own experience I can ensure you that tidyverse is all you need to carry out data analyses as neat, flexible and readable as possible. By the way, I believe these are the three most important goals for those who want to thrive as a data scientist. By neat I mean succinct code without any redundancy, a single block of code (not a bunch of loose objects) doing all the desired task. Flexible, in turn, means the code is general enough. It depends on the nature of the task at hand, so take it more like a philosophy. For example, does the solution still work if the data was updated with new values? Does it depend either on column names or the specific length of the vector? Finally, readable means that not only the understanding of the code is easy, but also that it’s straightforward to make adjustments to it if necessary. Most of the time writing neat, flexible and readable code takes longer and requires a dose of creativity, but it surely pays off. Moreover, with practice this will become more and more natural and the cost will get significantly lower. In the next sections we’ll review the most used functions to go through all the steps of everyday data analyses using core tidyverse packages designed to Import, Wrangle, Program and Plot. 2.2 Import 2.2.1 Reading from files readr is the package used to import data from flat files (CSV and TXT). The most generic function is read_delim as it has a large set of arguments that allow us to declare the structure of the data we want to import. In practice, we often use the read_csv which is a special case of the former with some predefined settings suitable for CSV files – the most obvious being the comma as the column delimiter. If you’re running RStudio IDE, I recommend you to click on Import Dataset at the Environment sheet (by default it’s located in the upper right panel) to manually define the appropriate settings for your file. We should never be encouraged to use windows, click on buttons or any kind of shortcuts provided by the IDE. This is the only exception I think is worth it. First, because it can be tedious to set many specific parameters via trial-and-error to correctly import your data. Second, because once you have the configuration done the code is displayed at the bottom of the window so you can copy it and paste it on your script and, eventually, get rid of this shortcut as you learn how things work. Lastly, you’ll usually import one single data set for each task and there’s not much to improve on this process. Then, Yes, it’s a big waste of time trying to understand each single argument. Just for the sake of illustration, let’s use readr to import the COVID data from Our World in Data we’ll use in Chapter 4. It’s a standard CSV file, so we can use read_csv without any additional parameter. We just need to provide the file path or URL: data_url &lt;- &#39;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv&#39; covid_data_from_url &lt;- readr::read_csv(data_url) covid_data_from_local &lt;- readr::read_csv(&#39;data/owid-covid-data.csv&#39;) Tidyverse also provides packages for reading other usual file types. For instance, we can use readxl to read MS Excel spreadsheets and haven to read data generated by popular statistical softwares (SPSS, Stata and SAS) in much the same way we did with readr for CSV files – you can check them at the Import Dataset dropdown menu. So we won’t waste time covering them. Rather, let’s use the rest of this section to talk about something very important and that usually gets less attention in books. 2.2.2 Reading from API Although the use of these traditional file formats are still widespread, organizations are increasingly switching to API’s to make data available to the public. This is a huge improvement since we’re able to customize our demand and retrieve only the data we need for the specific task. If you don’t know what an API is, think of it as an interface to establish a connection to a database. However, instead of reading all the content from this database you can inform your preferences like what columns you want, the desired observations (time range if it’s a time series) and so on. These preferences are embedded in a request and the right form to specify it is usually available in a documentation provided by the API maintainer. For example, the Federal Reserve Bank of St. Louis maintains a huge repository with hundreds of thousands of economic time series and we can import them using an API. The information on how to build the request is available in the documentation, where we can see a link with instructions on how to ‘Get the observations or data values for an economic data series’. There, we find an example of a request for the US Real Gross National Product: https://api.stlouisfed.org/fred/series/observations?series_id=GNPCA&amp;api_key=abcdefghijklmnopqrstuvwxyz123456&amp;file_type=json We can break this request to see its parts in more detail: The static URL to access the Observations section in the API. \\[ \\underbrace{\\text{https://api.stlouisfed.org/fred/series/observations?}}_\\text{Static: API URL} \\] The series ID which identifies what series we want. \\[ \\underbrace{\\text{series_id=GNPCA}}_\\text{Param. #1: Series ID} \\] The API Key. We must create an account and require a personal key in order to retrieve data from the API. This one is an example for illustrative purposes only. \\[ \\underbrace{\\text{api_key=abcdefghijklmnopqrstuvwxyz123456}}_\\text{Param. #2: API Key} \\] The file type for the output. There are several types, but I’d rather working with JSON. \\[ \\underbrace{\\text{file_type=json}}_\\text{Param. #3: File Type} \\] Note that all the parameters following the static part is separated by an &amp;. Then, if we want to add any extra parameter it should be placed right after an &amp;. Suppose we want to read monthly data for the US Consumer Price Index (CPI). The series_id is CPALTT01USM657N. Besides, we’d only like to read data as of January 2010. How can we do so? There’s a parameter observation_start which defines the start of the observation period (YYYY-MM-DD format). The code below creates a separate object for each part of the request. Then, we use the glue function from the homonymous package to merge the pieces into a single string adding the appropriate &amp;. Of course, we could create the complete string all at once. But, creating separate objects makes it easier to find and edit values as well as to transform this task into a function if we had to read other data from this source frequently (more on this later in Chapter 3). api_url &lt;- &#39;https://api.stlouisfed.org/fred/series/observations?&#39; api_key &lt;- &#39;bc0b91e1c6fa2cab4d4f180c7965edfa&#39; api_series_id &lt;- &#39;CPALTT01USM657N&#39; obs_start &lt;- &#39;2010-01-01&#39; api_filetype &lt;- &#39;json&#39; api_request &lt;- glue::glue(&#39;{api_url}series_id={api_series_id}&amp;observation_start={obs_start}&amp;api_key={api_key}&amp;file_type={api_filetype}&#39;) Now, we use the httr package to connect to the API, send the request and get the content. The other steps transform the content from JSON to a standard R object (a list) and then convert it to a tibble (the tidyverse’s improved format for data frames). The element observations is specific for this API, so if we imported data from another source we’d have to check in which element the desired content would be stored. library(magrittr) cpi_request &lt;- httr::GET(url = api_request) cpi_content &lt;- httr::content(cpi_request, as = &#39;text&#39;) cpi_list &lt;- jsonlite::fromJSON(cpi_content, flatten = FALSE) cpi_tbl &lt;- cpi_list[[&#39;observations&#39;]] %&gt;% tibble::as_tibble() cpi_tbl ## # A tibble: 144 × 4 ## realtime_start realtime_end date value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2022-02-01 2022-02-01 2010-01-01 0.34174735701485 ## 2 2022-02-01 2022-02-01 2010-02-01 0.024920738207648 ## 3 2022-02-01 2022-02-01 2010-03-01 0.410628353657108 ## 4 2022-02-01 2022-02-01 2010-04-01 0.173688491069743 ## 5 2022-02-01 2022-02-01 2010-05-01 0.0775197354237721 ## 6 2022-02-01 2022-02-01 2010-06-01 -0.0976267084673888 ## 7 2022-02-01 2022-02-01 2010-07-01 0.0211043057371509 ## 8 2022-02-01 2022-02-01 2010-08-01 0.138066427840807 ## 9 2022-02-01 2022-02-01 2010-09-01 0.0581736230715569 ## 10 2022-02-01 2022-02-01 2010-10-01 0.124519888847696 ## # … with 134 more rows This concludes our section on how to import data. If you’re struggling with other types of data, Tidyverse’s official website provides a comprehensive list of all the supported file formats and the respective packages used to handle them. 2.3 Wrangle This is by far the most important section in this Chapter. The main goal here is to provide a general sense of how to get raw data ready to use. For this, we’ll focus on the roles of the main functions inside the core packages rather than the idiosyncrasies and generalizations of each one. A deeper approach will be taken in the next chapters. 2.3.1 Data manipulation Let’s use the COVID data we read in the last section. Starting with the glimpse function to have a grasp of the data, we can see some useful information such as the number of rows and columns, as well as columns’ names and classes (whether they’re character, double, etc). covid_data &lt;- readr::read_csv(&#39;data/owid-covid-data.csv&#39;) covid_data %&gt;% dplyr::glimpse() ## Rows: 148,800 ## Columns: 67 ## $ iso_code &lt;chr&gt; &quot;AFG&quot;, &quot;AFG&quot;, &quot;AFG&quot;, &quot;AFG&quot;,… ## $ continent &lt;chr&gt; &quot;Asia&quot;, &quot;Asia&quot;, &quot;Asia&quot;, &quot;As… ## $ location &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan… ## $ date &lt;date&gt; 2020-02-24, 2020-02-25, 20… ## $ total_cases &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, … ## $ new_cases &lt;dbl&gt; 5, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ new_cases_smoothed &lt;dbl&gt; NA, NA, NA, NA, NA, 0.714, … ## $ total_deaths &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_deaths &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_deaths_smoothed &lt;dbl&gt; NA, NA, NA, NA, NA, 0, 0, 0… ## $ total_cases_per_million &lt;dbl&gt; 0.126, 0.126, 0.126, 0.126,… ## $ new_cases_per_million &lt;dbl&gt; 0.126, 0.000, 0.000, 0.000,… ## $ new_cases_smoothed_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, 0.018, … ## $ total_deaths_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_deaths_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_deaths_smoothed_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, 0, 0, 0… ## $ reproduction_rate &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ icu_patients &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ icu_patients_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ hosp_patients &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ hosp_patients_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ weekly_icu_admissions &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ weekly_icu_admissions_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ weekly_hosp_admissions &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ weekly_hosp_admissions_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_tests &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ total_tests &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ total_tests_per_thousand &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_tests_per_thousand &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_tests_smoothed &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_tests_smoothed_per_thousand &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ positive_rate &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ tests_per_case &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ tests_units &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ total_vaccinations &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ people_vaccinated &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ people_fully_vaccinated &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ total_boosters &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_vaccinations &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_vaccinations_smoothed &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ total_vaccinations_per_hundred &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ people_vaccinated_per_hundred &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ people_fully_vaccinated_per_hundred &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ total_boosters_per_hundred &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_vaccinations_smoothed_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_people_vaccinated_smoothed &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ new_people_vaccinated_smoothed_per_hundred &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ stringency_index &lt;dbl&gt; 8.33, 8.33, 8.33, 8.33, 8.3… ## $ population &lt;dbl&gt; 39835428, 39835428, 3983542… ## $ population_density &lt;dbl&gt; 54.422, 54.422, 54.422, 54.… ## $ median_age &lt;dbl&gt; 18.6, 18.6, 18.6, 18.6, 18.… ## $ aged_65_older &lt;dbl&gt; 2.581, 2.581, 2.581, 2.581,… ## $ aged_70_older &lt;dbl&gt; 1.337, 1.337, 1.337, 1.337,… ## $ gdp_per_capita &lt;dbl&gt; 1803.987, 1803.987, 1803.98… ## $ extreme_poverty &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ cardiovasc_death_rate &lt;dbl&gt; 597.029, 597.029, 597.029, … ## $ diabetes_prevalence &lt;dbl&gt; 9.59, 9.59, 9.59, 9.59, 9.5… ## $ female_smokers &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ male_smokers &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ handwashing_facilities &lt;dbl&gt; 37.746, 37.746, 37.746, 37.… ## $ hospital_beds_per_thousand &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.… ## $ life_expectancy &lt;dbl&gt; 64.83, 64.83, 64.83, 64.83,… ## $ human_development_index &lt;dbl&gt; 0.511, 0.511, 0.511, 0.511,… ## $ excess_mortality_cumulative_absolute &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ excess_mortality_cumulative &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ excess_mortality &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… ## $ excess_mortality_cumulative_per_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA,… We’re usually not interested in all of these data, so the first couple of tasks we’d like to perform is to filter the relevant categories and select the desired columns. For example, we could be interested in analyzing new COVID cases (column new_cases) only in Brazil (rows equal to Brazil in column location). Furthermore, we’d like to get rid of duplicate rows if there’s any. One of the great contributions of tidyverse is to assign names to the functions according to the actions they perform – many are admittedly SQL-inspired. distinct select only unique rows from a data set, thus dropping duplicates. select picks variables based on their names, whereas filter retains the rows which satisfy a given condition – or a set of conditions. Conditions are sentences which returns TRUE or FALSE. It’s straightforward to think of conditions using logical operators, such as ==, &gt;, &lt;, etc. Nevertheless, there’s a bunch of expressions in R which return TRUE or FALSE. Moreover, we can always create our own condition to suit the work purpose. We’ll see many examples throughout this book. We can use the code below to generate the desired subset of the data: covid_data_01 &lt;- covid_data %&gt;% dplyr::distinct() %&gt;% dplyr::select(date, continent, location, new_cases) %&gt;% dplyr::filter(location == &#39;Brazil&#39;) Next, we’d probably need to create additional columns. For instance, suppose that actual new cases in Brazil are much higher than reported because the country doesn’t have enough tests and a conservative estimate points to a number of, at least, twice the official count. So we’d like to add a column which is twice the value of the original one representing our guess of the real situation. In addition, we’d also like to create a column to indicate the dominant strain at each period of time. We know that the Delta strain took over Gamma by the end of July 2021 and, then, Omicron took over Delta in the beginning of 2022. mutate can be used to create new variables as a function of existing variables, but not necessarily. Also, it can be used to modify existing variables as new variables overwrites existing variables of the same name. case_when is another SQL-inspired function used inside mutate to create a new variable based on conditions. It’s worth noting that it’ll return NA if no condition is met. In this case, a useful workaround is to define an extra condition as TRUE ~ value, thus assigning a value to all the unmet conditions – think of this as an else condition. covid_data_02 &lt;- covid_data_01 %&gt;% dplyr::mutate( real_new_cases = new_cases*2, dominant_strain = dplyr::case_when( date &lt;= &#39;2021-07-31&#39; ~ &#39;Gamma&#39;, date &gt; &#39;2021-07-31&#39; &amp; date &lt;= &#39;2021-12-31&#39; ~ &#39;Delta&#39;, date &gt; &#39;2021-12-31&#39; &amp; date &lt;= &#39;2022-02-01&#39; ~ &#39;Omicron&#39;, TRUE ~ &quot;We don&#39;t know&quot; ) ) So far we’ve worked on a single group of data: new cases in Brazil. However, we usually have many categories to work on. We might be interested in analyzing new cases in all European countries. In this case, we’ll need the group_by function, which allows us to perform any operation by group. group_by is often used in conjunction with mutate or summarise to create new data for each group. The latter uses aggregate functions (mean, max, min, etc) to produce a summary of the data. For example, suppose we want to know which European country recorded the highest number of new covid cases in a single day by the end of 2021. This might be achieved by grouping the data by location and then using summarise with max. In addition, we can use arrange to sort the rows by value (we use desc to sort in descending order). Don’t forget to ungroup data as soon as you no longer need to perform operations by group. covid_data_03 &lt;- covid_data %&gt;% dplyr::distinct() %&gt;% dplyr::filter(continent == &#39;Europe&#39;, date &lt;= &#39;2021-12-31&#39;) %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(max_new_cases = max(new_cases)) %&gt;% dplyr::ungroup() %&gt;% dplyr::arrange(desc(max_new_cases)) Note that some countries such as Spain, Portugal and France returned NA. This happened because they probably have any missing value. We could easily ignore it by passing the argument na.rm = TRUE to the max function. However, another problem would arise: countries with no data on new cases would return -Inf. To get around these two issues, we can filter out all the missing values in the data set (the ! before the condition works as a negation). In this case removing the missing values isn’t a problem, but be aware that for some tasks this may influence the outcome. covid_data_04 &lt;- covid_data %&gt;% dplyr::distinct() %&gt;% dplyr::filter(continent == &#39;Europe&#39;, date &lt;= &#39;2021-12-31&#39;, !is.na(new_cases)) %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(max_new_cases = max(new_cases)) %&gt;% dplyr::ungroup() covid_data_04 ## # A tibble: 49 × 2 ## location max_new_cases ## &lt;chr&gt; &lt;dbl&gt; ## 1 Albania 1239 ## 2 Andorra 696 ## 3 Austria 15809 ## 4 Belarus 2097 ## 5 Belgium 47836 ## 6 Bosnia and Herzegovina 3755 ## 7 Bulgaria 7560 ## 8 Croatia 7315 ## 9 Cyprus 1909 ## 10 Czechia 27793 ## # … with 39 more rows In addition, we might want to know not only what the highest numbers were but when they did occur (the peak date). Since the summarise function is designed to return a single value, we must use an expression which returns a single value. If we used date = max(date), we’d keep the most recent date for the data in each country. Definitely, that’s not what we want. So a good way to address this issue is to combine a subset operation with a condition inside. In simpler terms, we’ll subset from the column date the observation where new cases were at their high. Since we can have multiple dates which satisfy this condition, we’ll keep the most recent one (the max of them). covid_data_05 &lt;- covid_data %&gt;% dplyr::distinct() %&gt;% dplyr::filter(continent == &#39;Europe&#39;, date &lt;= &#39;2021-12-31&#39;, !is.na(new_cases)) %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(max_new_cases = max(new_cases), peak_date = date[which(new_cases == max_new_cases)], peak_date = max(peak_date)) %&gt;% dplyr::ungroup() %&gt;% dplyr::arrange(peak_date) %&gt;% dplyr::slice(1:10) covid_data_05 ## # A tibble: 10 × 3 ## location max_new_cases peak_date ## &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Vatican 7 2020-10-15 ## 2 Vatican 7 2020-10-15 ## 3 Luxembourg 1967 2020-11-02 ## 4 Montenegro 874 2020-11-04 ## 5 Italy 40902 2020-11-13 ## 6 Sweden 32485 2020-12-29 ## 7 Gibraltar 180 2020-12-30 ## 8 Lithuania 3984 2020-12-30 ## 9 Ireland 8227 2021-01-08 ## 10 Spain 93822 2021-01-25 # Verificar depois por que Vaticano aparece com 2 entradas In case it went unnoticed, the previous code showed a really nice feature of mutate/summarise: you can use a variable you’ve just created in a subsequent task inside the same calling. In this example, we used the maximum number of new cases (first variable created) as a reference to obtain the dates when it occurred (second variable created) and them picked only the maximum date from a possibly set of dates (third variable created), all of this inside the same summarise calling as if all steps were performed sequentially. Finally, we used arrange to sort countries by their peak date (this time by ascending order, the default) and then slice to subset only the first ten countries out of the forty-nine in our data set. Falar dos joins, bind-rows/cols 2.3.2 Data layout We’ve walked through all the main functions of dplyr. Now, we turn to the tidyr package. According to tidyverse’s website, the goal of tidyr is to help us create tidy data. It means a data set where every column is a variable; every row is an observation; and each cell is a single value. This is more like a convention created by tidyverse so that “you’ll spend less time fighting with the tools and more time working on your analysis”. Tidy data is also known as wide format – since it increases the number of columns and decreases the number of rows –, as opposed to the long format, where data are stacked increasing the number of rows and decreasing the number of columns. It took me a while before I could tell if a data set was in wide or long format, so don’t worry if it’s not so clear right now. Perhaps a more direct way of thinking about this distinction is to ask yourself: Is all the information contained in a single cell?. If yes, it’s wide format. If not, it’s long format. For example, is the covid data set in wide or long format? If we took a single cell from the new_cases column, does it convey all the information for this variable? No, it doesn’t. We know the number of new cases in a given date, but we don’t know the country that value refers to – is this from Germany? Nigeria? Chile? We can use tidyr::pivot_wider to convert from long to wide format. The syntax is very easy to understand: names_from is the column we want to widen, whereas values_from is the column containing the observations that will fill each cell. id_cols is an optional argument used to declare the set of columns that uniquely identifies each observation. In practice, it drops all the other columns in the data set. Hence, if we want to keep all the other variables, just skip it. covid_wide_01 &lt;- covid_data %&gt;% tidyr::pivot_wider(id_cols = c(&#39;date&#39;, &#39;location&#39;), names_from = &#39;location&#39;, values_from = &#39;new_cases&#39;) covid_wide_01 ## # A tibble: 720 × 239 ## date Afghanistan Africa Albania Algeria Andorra Angola Anguilla ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-02-24 5 0 NA NA NA NA NA ## 2 2020-02-25 0 1 NA 1 NA NA NA ## 3 2020-02-26 0 0 NA 0 NA NA NA ## 4 2020-02-27 0 0 NA 0 NA NA NA ## 5 2020-02-28 0 1 NA 0 NA NA NA ## 6 2020-02-29 0 0 NA 0 NA NA NA ## 7 2020-03-01 0 1 NA 0 NA NA NA ## 8 2020-03-02 0 4 NA 2 1 NA NA ## 9 2020-03-03 0 3 NA 2 0 NA NA ## 10 2020-03-04 0 10 NA 7 0 NA NA ## # … with 710 more rows, and 231 more variables: Antigua and Barbuda &lt;dbl&gt;, ## # Argentina &lt;dbl&gt;, Armenia &lt;dbl&gt;, Aruba &lt;dbl&gt;, Asia &lt;dbl&gt;, Australia &lt;dbl&gt;, ## # Austria &lt;dbl&gt;, Azerbaijan &lt;dbl&gt;, Bahamas &lt;dbl&gt;, Bahrain &lt;dbl&gt;, ## # Bangladesh &lt;dbl&gt;, Barbados &lt;dbl&gt;, Belarus &lt;dbl&gt;, Belgium &lt;dbl&gt;, ## # Belize &lt;dbl&gt;, Benin &lt;dbl&gt;, Bermuda &lt;dbl&gt;, Bhutan &lt;dbl&gt;, Bolivia &lt;dbl&gt;, ## # Bonaire Sint Eustatius and Saba &lt;dbl&gt;, Bosnia and Herzegovina &lt;dbl&gt;, ## # Botswana &lt;dbl&gt;, Brazil &lt;dbl&gt;, British Virgin Islands &lt;dbl&gt;, Brunei &lt;dbl&gt;, … Notice that now we have 239 columns rather than 67 of the original data set with each cell conveying the whole information: new cases for a given date for a specific country. So, what if we wanted to have both new_cases and new_deaths columns in wide form? We just need to provide a vector with the desired variables in values_from. By default, the new columns are named according to the following pattern: variable_location. Since the variables names already contain an underscore, it’s a good idea to set the names separator to another character. This is because many functions have a “names separator” argument, thus it’s much easier to use it if we have a unique separator character. Otherwise, we’d have to use regular expression to inform the specific position of the character – for example, whether it’s the first or the second underscore. covid_wide_02 &lt;- covid_data %&gt;% tidyr::pivot_wider(id_cols = c(&#39;date&#39;, &#39;location&#39;), names_from = &#39;location&#39;, values_from = c(&#39;new_cases&#39;, &#39;new_deaths&#39;), names_sep = &#39;-&#39;) covid_wide_02 ## # A tibble: 720 × 477 ## date `new_cases-Afghanistan` `new_cases-Africa` `new_cases-Albania` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-02-24 5 0 NA ## 2 2020-02-25 0 1 NA ## 3 2020-02-26 0 0 NA ## 4 2020-02-27 0 0 NA ## 5 2020-02-28 0 1 NA ## 6 2020-02-29 0 0 NA ## 7 2020-03-01 0 1 NA ## 8 2020-03-02 0 4 NA ## 9 2020-03-03 0 3 NA ## 10 2020-03-04 0 10 NA ## # … with 710 more rows, and 473 more variables: new_cases-Algeria &lt;dbl&gt;, ## # new_cases-Andorra &lt;dbl&gt;, new_cases-Angola &lt;dbl&gt;, new_cases-Anguilla &lt;dbl&gt;, ## # new_cases-Antigua and Barbuda &lt;dbl&gt;, new_cases-Argentina &lt;dbl&gt;, ## # new_cases-Armenia &lt;dbl&gt;, new_cases-Aruba &lt;dbl&gt;, new_cases-Asia &lt;dbl&gt;, ## # new_cases-Australia &lt;dbl&gt;, new_cases-Austria &lt;dbl&gt;, ## # new_cases-Azerbaijan &lt;dbl&gt;, new_cases-Bahamas &lt;dbl&gt;, ## # new_cases-Bahrain &lt;dbl&gt;, new_cases-Bangladesh &lt;dbl&gt;, … Our new data set expanded to 477 columns. As we create more and more columns to arrange more variables it becomes harder to perform some simple tasks. For example, it’s much easier to use group_by with mutate or summarise when we have the data set in long format. In addition, using filter is generally easier than using a conditional select when we want to keep only the relevant data. In summary, long format may be preferable over wide format when the data set contains more than one grouping variable or we want to work on more than one variable. Besides, long format data sets are ideal for plotting with ggplot2 package as we’ll see later. Therefore, it’s not unusual to convert a data set from wide to long format. The syntax is very similar to what we saw earlier when converting from long to wide format. The unique difference is in cols argument, used to declare what columns we want to stack. However, since wide data sets usually have a large number of columns and we’re often interested in putting all of them in long format, it’s much easier to declare what columns we want to leave out (-). covid_long_01 &lt;- covid_wide_02 %&gt;% tidyr::pivot_longer(cols = -&#39;date&#39;, names_to = c(&#39;variable&#39;, &#39;location&#39;), values_to = &#39;value&#39;, names_sep = &#39;-&#39;) covid_long_01 ## # A tibble: 342,720 × 4 ## date variable location value ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020-02-24 new_cases Afghanistan 5 ## 2 2020-02-24 new_cases Africa 0 ## 3 2020-02-24 new_cases Albania NA ## 4 2020-02-24 new_cases Algeria NA ## 5 2020-02-24 new_cases Andorra NA ## 6 2020-02-24 new_cases Angola NA ## 7 2020-02-24 new_cases Anguilla NA ## 8 2020-02-24 new_cases Antigua and Barbuda NA ## 9 2020-02-24 new_cases Argentina NA ## 10 2020-02-24 new_cases Armenia NA ## # … with 342,710 more rows Note that now even the variables (new_cases and new_deaths) are stored in a single column (variable). This is probably an abuse of language, but I call this a complete long format – as opposed to the original format where the data set was only partially in the long format (the variables were indeed in wide format). For most applications, I think this is the best way to organize the data. Converting a data set between wide and long formats might not completely solve our problem. Sometimes, two pieces of information are merged in a single column. For example, suppose that the location column had both the continent and country names instead of only the country as in the original data set. We’ll call this data set covid_loc_cont ## # A tibble: 139,819 × 4 ## location date new_cases new_deaths ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan_Asia 2020-02-24 5 NA ## 2 Afghanistan_Asia 2020-02-25 0 NA ## 3 Afghanistan_Asia 2020-02-26 0 NA ## 4 Afghanistan_Asia 2020-02-27 0 NA ## 5 Afghanistan_Asia 2020-02-28 0 NA ## 6 Afghanistan_Asia 2020-02-29 0 NA ## 7 Afghanistan_Asia 2020-03-01 0 NA ## 8 Afghanistan_Asia 2020-03-02 0 NA ## 9 Afghanistan_Asia 2020-03-03 0 NA ## 10 Afghanistan_Asia 2020-03-04 0 NA ## # … with 139,809 more rows This is undesirable since we can no longer use group by or filter over continent alone, for instance. Hence, the best practice is to have a single column for each variable. This can be easily achieved using the separate function, with a highly self-explanatory syntax. covid_separate_01 &lt;- covid_loc_cont %&gt;% dplyr::distinct() %&gt;% tidyr::separate(col = &#39;location&#39;, into = c(&#39;location&#39;, &#39;continent&#39;), sep = &#39;_&#39;) covid_separate_01 ## # A tibble: 139,819 × 5 ## location continent date new_cases new_deaths ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2020-02-24 5 NA ## 2 Afghanistan Asia 2020-02-25 0 NA ## 3 Afghanistan Asia 2020-02-26 0 NA ## 4 Afghanistan Asia 2020-02-27 0 NA ## 5 Afghanistan Asia 2020-02-28 0 NA ## 6 Afghanistan Asia 2020-02-29 0 NA ## 7 Afghanistan Asia 2020-03-01 0 NA ## 8 Afghanistan Asia 2020-03-02 0 NA ## 9 Afghanistan Asia 2020-03-03 0 NA ## 10 Afghanistan Asia 2020-03-04 0 NA ## # … with 139,809 more rows The only caveat to all this simplicity is when we have non-trivial separators. For example, imagine that we had no underscore to separate location from continent. ## # A tibble: 139,819 × 4 ## location date new_cases new_deaths ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AfghanistanAsia 2020-02-24 5 NA ## 2 AfghanistanAsia 2020-02-25 0 NA ## 3 AfghanistanAsia 2020-02-26 0 NA ## 4 AfghanistanAsia 2020-02-27 0 NA ## 5 AfghanistanAsia 2020-02-28 0 NA ## 6 AfghanistanAsia 2020-02-29 0 NA ## 7 AfghanistanAsia 2020-03-01 0 NA ## 8 AfghanistanAsia 2020-03-02 0 NA ## 9 AfghanistanAsia 2020-03-03 0 NA ## 10 AfghanistanAsia 2020-03-04 0 NA ## # … with 139,809 more rows How could we manage to separate them? Ideally, we should provide a regular expression (regex) to match the pattern we want to use to split the string into location and continent (the sep argument works with regex). At first glance, a natural choice would be to split the string as of the second uppercase letter. However, this would fail for several countries: United States, New Zealand and many others. At the time this book is being written, I confess I don’t know how to build this specific regular expression. More often than not, the direct solution is the most difficult (or we simply don’t know how) to implement and so we have to think about alternative answers. Data science is all about this. My workaround for this case is rather ingenious: I know how to subset the first letter of a string. In addition, I also know how to catch a string starting by a uppercase letter. covid_separate_02 &lt;- covid_loc_cont2 %&gt;% tidyr::separate(col = &#39;location&#39;, into = c(&#39;location&#39;, &#39;continent&#39;), sep = &#39;(?=[A-Z](?!.*[A-Z].*))&#39;) %&gt;% tidyr::separate(col = &#39;location&#39;, into = c(&#39;location&#39;, &#39;cont_part1&#39;), sep = &#39;(?=[A-Z](?!.*[A-Z].*))&#39;) Falar de unite, separate e nest Falar de tidyr, stringr, purrr, ggpplot. Uma seção dedicada a missing values: como visualizar e preencher? "],
["google.html", "Chapter 3 Google Mobility 3.1 Importing data 3.2 Preparing the data 3.3 Plot information 3.4 From code to function", " Chapter 3 Google Mobility For this exercise, we will use the Google Mobility data. Google started releasing this data on a daily basis right after the COVID outbreak spread across the world by mid-February 2020. Since then this data has been widely used for different purposes: from assessing/measuring economic activity to designing public policies. In spite of being a well-organized dataset, it does offer the opportunity to do real-life data wrangling and to explore good practices from importing through visualizing. 3.1 Importing data Google offers two ways of downloading its mobility data: you can either get a unique .csv file with all the available countries or you can get a .zip file with a separate .csv file for each country. We will stick with the latter for two simple reasons. First, it is much more challenging to import many files at once and bind them together. Second, this kind of work comes up very often in everyday work. We begin by downloading the .zip file to the data folder. download.file( url = &#39;https://www.gstatic.com/covid19/mobility/Region_Mobility_Report_CSVs.zip&#39;, destfile = &#39;data/Region_Mobility_Report_CSVs.zip&#39; ) Now suppose we want to analyze a small group of countries. In this case, a better approach is to only import the .csv corresponding to these countries and then bind them together. After a brief inspection of the .zip file, we can see a clear pattern in filenames: year_country-code_Region_Mobility_Report.csv. For example, the file containing data for Brazil in 2021 is: 2021_BR_Region_Mobility_Report.csv. So, our first task is to produce a vector with the desired filenames. This vector will be used later to extract the corresponding .csv from the .zip file. If you have enough experience with data science you may have already figured out a solution. Before taking it forward, stop for a while and think: is it general enough? What if Google kept releasing this data for years in the future, would your solution still work? In order to not get ourselves caught in this trap, we invoke Sys.Date() to recover the current year and use it as the end point of our sequence of years. Whether you are reading this book in 2022 or in 2030 and Google still releases mobility data with filenames following the same pattern, then you can safely use the solution below. library(lubridate) library(tidyverse) countries_codes &lt;- c(&#39;BR&#39;, &#39;US&#39;, &#39;DE&#39;, &#39;ZA&#39;, &#39;SG&#39;, &#39;AU&#39;) years &lt;- seq(from = 2020, to = lubridate::year(Sys.Date()), by = 1) google_filenames &lt;- purrr::cross2(years, countries_codes) %&gt;% purrr::map_chr(.f = function(x){ x %&gt;% glue::glue_collapse(sep = &#39;_&#39;) %&gt;% glue::glue(&#39;_Region_Mobility_Report.csv&#39;) }) The purrr::cross2 function creates all the combinations between codes and years we need to replicate the first part of the filenames. The final part is static, so it’s just a matter of pasting this piece into each element of the vector. We can check the result below: ## [1] &quot;2020_BR_Region_Mobility_Report.csv&quot; &quot;2021_BR_Region_Mobility_Report.csv&quot; ## [3] &quot;2022_BR_Region_Mobility_Report.csv&quot; &quot;2020_US_Region_Mobility_Report.csv&quot; ## [5] &quot;2021_US_Region_Mobility_Report.csv&quot; &quot;2022_US_Region_Mobility_Report.csv&quot; ## [7] &quot;2020_DE_Region_Mobility_Report.csv&quot; &quot;2021_DE_Region_Mobility_Report.csv&quot; ## [9] &quot;2022_DE_Region_Mobility_Report.csv&quot; &quot;2020_ZA_Region_Mobility_Report.csv&quot; ## [11] &quot;2021_ZA_Region_Mobility_Report.csv&quot; &quot;2022_ZA_Region_Mobility_Report.csv&quot; ## [13] &quot;2020_SG_Region_Mobility_Report.csv&quot; &quot;2021_SG_Region_Mobility_Report.csv&quot; ## [15] &quot;2022_SG_Region_Mobility_Report.csv&quot; &quot;2020_AU_Region_Mobility_Report.csv&quot; ## [17] &quot;2021_AU_Region_Mobility_Report.csv&quot; &quot;2022_AU_Region_Mobility_Report.csv&quot; To finish up, we now resort to the purrr::map function to extract each filename from the .zip file. At this stage, I would like to draw your attention for something very important. Remember that we are ultimately interested in binding together the twelve .csv files in google_filenames. Since Google’s files contain columns for both country and date, we could safely use purrr::map_dfr to automatically stack the data – it would save us a few lines of code. However, it could well be that these files do not contain identifying columns – this information being only in the filename. It happens more often than not in real-life applications. So, if we naively stacked these files we would never be able to distinguish which country or date period each piece of the resulting data frame refers to. Therefore, a better strategy in this case is: Using the purrr::map function to import each file as an element in a list. Assigning meaningful names for each element of that list with magrittr::set_names. Invoking the plyr::ldply function to stack them. The plyr::ldply function is very convenient here because it carries the names of the elements in the list into the resulting data frame as a new column. In addition, it also has several other useful features such as applying a generic function to each element of the list before stacking it and parallel processing. In this example, the filenames contain both the country code and year for each dataset. Thankfully, we have a very simple pattern and we can extract the relevant information from the first seven characters of each element in our vector google_filenames. More complicated patterns would require the use of regular expressions (regex). We are now ready to perform our stacking safely. mobility_data &lt;- purrr::map( .x = google_filenames, .f = function(x){ readr::read_csv(unz(&#39;data/Region_Mobility_Report_CSVs.zip&#39;, x)) }) %&gt;% magrittr::set_names(stringr::str_sub(google_filenames, start = 1, end = 7)) %&gt;% plyr::ldply(.id = &#39;year_country&#39;) Below the first four columns and five rows of the resulting data frame. We could make things as neat as possible by using the purrr::separate function to separate the year_country column into two columns: year and country. But since it is not a necessary step here (we already have these information from other columns), we will leave it as an exercise. ## year_country country_region_code country_region sub_region_1 ## 1 2020_BR BR Brazil &lt;NA&gt; ## 2 2020_BR BR Brazil &lt;NA&gt; ## 3 2020_BR BR Brazil &lt;NA&gt; ## 4 2020_BR BR Brazil &lt;NA&gt; ## 5 2020_BR BR Brazil &lt;NA&gt; 3.2 Preparing the data Now the we have successfully imported the data for the selected countries, it is time to produce useful content. Let’s begin with a closer look on the structure of the dataset. I removed the year_country column since it was only for pedagogical purposes and we won’t need it. mobility_data %&gt;% dplyr::glimpse() ## Rows: 3,356,027 ## Columns: 15 ## $ country_region_code &lt;chr&gt; &quot;BR&quot;, &quot;BR&quot;, &quot;BR&quot;, &quot;… ## $ country_region &lt;chr&gt; &quot;Brazil&quot;, &quot;Brazil&quot;,… ## $ sub_region_1 &lt;chr&gt; NA, NA, NA, NA, NA,… ## $ sub_region_2 &lt;chr&gt; NA, NA, NA, NA, NA,… ## $ metro_area &lt;lgl&gt; NA, NA, NA, NA, NA,… ## $ iso_3166_2_code &lt;chr&gt; NA, NA, NA, NA, NA,… ## $ census_fips_code &lt;chr&gt; NA, NA, NA, NA, NA,… ## $ place_id &lt;chr&gt; &quot;ChIJzyjM68dZnAARYz… ## $ date &lt;date&gt; 2020-02-15, 2020-0… ## $ retail_and_recreation_percent_change_from_baseline &lt;dbl&gt; 5, 2, -2, -3, -1, 1… ## $ grocery_and_pharmacy_percent_change_from_baseline &lt;dbl&gt; 4, 3, 0, -1, -2, 7,… ## $ parks_percent_change_from_baseline &lt;dbl&gt; -5, -13, -12, -11, … ## $ transit_stations_percent_change_from_baseline &lt;dbl&gt; 8, 3, 9, 9, 8, 11, … ## $ workplaces_percent_change_from_baseline &lt;dbl&gt; 6, 0, 19, 15, 14, 1… ## $ residential_percent_change_from_baseline &lt;dbl&gt; 0, 1, -1, -1, -1, -… We can see our dataset has about 3.19 million rows and 15 columns. The most relevant information are stored in the columns ending with percent_change_from_baseline. These are precisely the measures of mobility for categorized places. The other columns of interest are those containing region and, of course, the column date. I recommend you to set some time aside to explore the dataset. You will notice that the sub_region_* columns refer to sub-national breakdowns such as states and municipalities. They are both NA for the national level. Suppose our ultimate goal is to have a plot with the average mobility of the categories for each country in the national-level. We know in advance that it’s very likely that a strong seasonal pattern is present. For example, mobility in workplaces should be higher during weekdays and lower on weekends. The opposite should be true for parks. We will address the issue of seasonality later in Chapter #. For now, it’s enough to know that creating a 7-days rolling mean of the original time series does the trick. Finally, we need to invert the residential mobility since a higher (lower) residential mobility means a lower (higher) mobility elsewhere. So, if we are to aggregate all the mobility categories into one single measure (the average) they must point to the same direction. Hence, our task is to produce a data frame with only the relevant variables. This involves, for each country, the following sequence of actions: Filter the national data. Invert the direction of the residential mobility (change the sign). Transform each mobility category column into a 7-days moving average. Create a column with the average mobility of categories. Remove the irrelevant variables. This should not be quite a challenge and we can accomplish it with a few lines of code using the right features from dplyr (1.0.0 or higher) package. I consider Items 3 and 4 the most important because we are tempted to offer a cumbersome solution that can be easily avoided with the proper tools. But before jumping to the best approach. let’s figure out how an old-fashioned approach might look like for Item 3. Using the RcppRoll::roll_meanr to compute 7-days rolling means, our first solution could be something like this: mutate_try1 &lt;- mobility_data %&gt;% dplyr::group_by(country_region) %&gt;% dplyr::arrange(date) %&gt;% dplyr::mutate( retail_and_recreation_percent_change_from_baseline = RcppRoll::roll_meanr(retail_and_recreation_percent_change_from_baseline, 7), grocery_and_pharmacy_percent_change_from_baseline = RcppRoll::roll_meanr(grocery_and_pharmacy_percent_change_from_baseline, 7), parks_percent_change_from_baseline = RcppRoll::roll_meanr(parks_percent_change_from_baseline, 7), transit_stations_percent_change_from_baseline = RcppRoll::roll_meanr(transit_stations_percent_change_from_baseline, 7), workplaces_percent_change_from_baseline = RcppRoll::roll_meanr(workplaces_percent_change_from_baseline, 7), residential_percent_change_from_baseline = RcppRoll::roll_meanr(residential_percent_change_from_baseline, 7) ) This solution is terrible, nevertheless I come across it very often. Fortunately, we already have a way to avoid it. The first step towards a better solution would be to use dplyr::across to replace the variable name in the right-hand side by .x. This will eliminate part of the redundancies. mutate_try2 &lt;- mobility_data %&gt;% dplyr::group_by(country_region) %&gt;% dplyr::arrange(date) %&gt;% dplyr::mutate( across(retail_and_recreation_percent_change_from_baseline, ~RcppRoll::roll_meanr(.x, 7)), across(grocery_and_pharmacy_percent_change_from_baseline, ~RcppRoll::roll_meanr(.x, 7)), across(parks_percent_change_from_baseline, ~RcppRoll::roll_meanr(.x, 7)), across(transit_stations_percent_change_from_baseline, ~RcppRoll::roll_meanr(.x, 7)), across(workplaces_percent_change_from_baseline, ~RcppRoll::roll_meanr(.x, 7)), across(residential_percent_change_from_baseline, ~RcppRoll::roll_meanr(.x, 7)) ) Ok, we’ve made some progress in cutting part of the repetitions but we can certainly do better. Note that in this case the variables we are interested in show a clear pattern: they all end with percent_change_from_baseline or simply baseline. We can take advantage of this to further improve our solution using select helpers. These are expressions that can be used to refer to specific patterns or make generalizations. For instance, here we could use the select helper ends_with to create the 7-days rolling mean for all the variables ending with baseline. In addition, we can also use the argument .names to assign a glue-style name to the new variables: {.col} gets the column name and {.fun} gets the name of the function. This is great to identify which function we applied to each variable. Here, we can use a ma7d suffix which stands for moving-average 7-days. mutate_topsolution &lt;- mobility_data %&gt;% dplyr::group_by(country_region) %&gt;% dplyr::arrange(date) %&gt;% dplyr::mutate( across(ends_with(&#39;baseline&#39;), ~RcppRoll::roll_meanr(.x, 7), .names = &#39;{.col}_ma7d&#39;) ) The main lesson here is to avoid using variables names to compute the operations. Instead, whenever possible we must rely on the combination of across and select helpers. This avoids unnecessarily writing variables names so many times and therefore allows us to scale up the work. The same reasoning applies to Item 4. Can you see how? Remember that Item 4 asks us to create a column with the average mobility of categories. Well, all these columns we need to average end with baseline. So we don’t need to rewrite all the variables names to get a new column with the mean – we can resort to select helpers. The only difference is that now we need an operation over the rows rather than over the columns – we are averaging the values over the rows. We can accomplish it by using dplyr::rowwise. Roughly speaking, this function turns every row of the data frame into a single group. Then you can perform your calculation on that group (the row). In addition, we have to replace the across function by the c_across function. The c_across is simply the equivalent of across when we’re using rowwise mode. Remember to use dplyr::ungroup to turn row-wise off and get back to the default column-wise mode when you don’t need operations over rows anymore. Below the full solution for Items 1 to 5. mobility_prep &lt;- mobility_data %&gt;% dplyr::filter(is.na(sub_region_1)) %&gt;% dplyr::mutate(across(starts_with(&#39;residential&#39;), ~ -1*.x)) %&gt;% dplyr::group_by(country_region) %&gt;% dplyr::arrange(date) %&gt;% dplyr::mutate(across(ends_with(&#39;baseline&#39;), ~ RcppRoll::roll_meanr(.x, 7, na.rm = TRUE), .names = &#39;{.col}_ma7d&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(avg_mobility = mean(c_across(ends_with(&#39;ma7d&#39;)), na.rm = TRUE)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(date, country_region, ends_with(&#39;baseline&#39;), avg_mobility) 3.3 Plot information We have mobility data for six countries and we should now decide how to plot them. Time series are usually better presented as lines, but there are some choices to be made. The most important one is whether we should display all the countries on a single or separate graph. It depends on the purpose of the plot. If we are interested in observing the differences among countries in the same time period, then a single graph is a natural choice. On the other hand, if our goal is to observe in more detail the evolution in each country, then a separate plot is more convenient. Let’s stick with the latter in this example. We can easily do this using facet_wrap which is a very cool feature from ggplot. In this case we’re segmenting our plot by country, but we’re not constrained to segment it by only one variable. Besides, we can use the argument scales = 'free_y' to make the scales of each graph more adjusted to the limits of the data. It’s not desirable here as we want to make visual comparisons between countries straightforward. mobility_prep %&gt;% ggplot(aes(x = date, y = avg_mobility)) + geom_line() + facet_wrap(~ country_region) + labs(title = &#39;Average mobility in selected countries - % change from baseline&#39;, subtitle = &#39;7-days moving average&#39;, caption = &#39;Data source: Google&#39;) + geom_hline(yintercept = 0, linetype = 2) + theme_light() 3.4 From code to function We have the complete code to import, prepare and visualize the data. Perhaps this analysis will become part of our routine or that of a stakeholder. And if that happens, it’s very likely that we’ll need to look at other countries. So a good practice in this case is to convert our code into a function. Creating a function is highly recommended whenever we have a repeated action on a different set of arguments. Here, we can think of two arguments that we would like to change eventually: country and the window size of the rolling mean. Therefore our task is to gather all the code we have produced so far and to transform these two inputs into arguments of the function. Note, however, that converting code into a function raises some issues. For example, when writing the code we used a vector to import data for the selected countries. It’s not the most efficient approach, because each file has a significant size and thus the execution may be very slow. This is a truly concern when we’re writing a function, because functions are most of the time used to loop over a large set of arguments – many countries, for example. Hence, we would like to process this task in parallel rather than serially. Surely we can perform this parallel processing inside the function, but I always prefer to keep things simpler and more transparent. This means to write a function to plot only a single country and, if necessary, we can use purrr::map to get as many countries as we want – and (explicitly) in parallel. Other minor yet important issue is that in a function we have to use the arguments as inputs everywhere, not only in obvious places. For example, when preparing the data we included a ma7d to the column names to indicate they were transformed into 7-days rolling mean. This label was also used in many actions later – when computing the average mobility, in the plot subtitle, etc. Therefore, we need to ensure that this argument will be considered in those actions as well. To achieve this, we’ll use glue::glue() function to create custom labels. plot_mobility &lt;- function(country_code, ma_window){ library(lubridate) library(tidyverse) library(glue) # Import data countries_codes &lt;- country_code years &lt;- seq(from = 2020, to = lubridate::year(Sys.Date()), by = 1) google_filenames &lt;- purrr::cross2(years, countries_codes) %&gt;% purrr::map_chr(.f = function(x){ x %&gt;% glue::glue_collapse(sep = &#39;_&#39;) %&gt;% glue::glue(&#39;_Region_Mobility_Report.csv&#39;) }) mobility_data &lt;- purrr::map_dfr( .x = google_filenames, .f = function(x){ readr::read_csv(unz(&#39;data/Region_Mobility_Report_CSVs.zip&#39;, x)) }) # Prepare data mobility_prep &lt;- mobility_data %&gt;% dplyr::filter(is.na(sub_region_1)) %&gt;% dplyr::mutate(across(starts_with(&#39;residential&#39;), ~ -1*.x)) %&gt;% dplyr::group_by(country_region) %&gt;% dplyr::arrange(date) %&gt;% dplyr::mutate(across(ends_with(&#39;baseline&#39;), ~ RcppRoll::roll_meanr(.x, ma_window, na.rm = TRUE), .names = &#39;{.col}_ma{ma_window}d&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(avg_mobility = mean(c_across(ends_with(glue(&#39;ma{ma_window}d&#39;))), na.rm = TRUE)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(date, country_region, ends_with(&#39;baseline&#39;), avg_mobility) # Output: plot mobility_prep %&gt;% ggplot(aes(x = date, y = avg_mobility)) + geom_line() + labs(title = glue(&#39;Average mobility in {country_code} - % change from baseline&#39;), subtitle = glue(&#39;{ma_window}-days moving average&#39;), caption = &#39;Data source: Google&#39;) + geom_hline(yintercept = 0, linetype = 2) + theme_light() } We can now use the function plot_mobility to plot any country we want and with the desired window for the rolling mean. plot_mobility(&#39;BR&#39;, 14) Or we can use purrr::map and gridExtra to plot several countries. countries &lt;- c(&#39;BR&#39;, &#39;US&#39;, &#39;FR&#39;, &#39;DE&#39;) mobility_countries &lt;- purrr::map(.x = countries, .f = plot_mobility, 14) %&gt;% magrittr::set_names(countries) mobility_countries[[&#39;DE&#39;]] To finish up, we must keep in mind that a function that return a plot is not very flexible and maybe we should consider the output to be the processed data instead of the plot. This would enable us to customize the plot and to perform other analysis as well. "],
["owidCovid.html", "Chapter 4 Our World in Data COVID 4.1 Importing data 4.2 Preparing the data", " Chapter 4 Our World in Data COVID 4.1 Importing data In this Chapter, we’ll turn to the COVID dataset from the Our World in Data project. It contains a complete set of variables related to COVID for a large number of countries. We can download it directly from the project’s Github: library(tidyverse) data_url &lt;- &#39;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv&#39; covid_data &lt;- readr::read_csv(data_url) 4.2 Preparing the data Suppose we’re interested in producing a rank with the ten’s most and least fully vaccinated countries in the world. At this time, fully vaccinated can be thought of as an individual who got two doses – we won’t consider booster doses. So our first task is to select the appropriate data on vaccinations. Using dplyr::glimpse() to check the variables in this dataset, we see three variables are of interest: location, date and people_fully_vaccinated_per_hundred. Besides, it’s likely that several countries don’t have any data on vaccinations at all. This is the case of Vaticano, for example. We’d like to get rid of them, since they won’t make part of our ranking. What we need, then, is a condition which returns TRUE only if a given location has any non-NA value. vacc_data &lt;- covid_data %&gt;% dplyr::select(location, date, people_fully_vaccinated_per_hundred) %&gt;% dplyr::group_by(location) %&gt;% dplyr::filter(any(!is.na(people_fully_vaccinated_per_hundred))) %&gt;% dplyr::ungroup() Since we only need the last figures for each country, we can filter the most recent observations. vacc_data_last &lt;- vacc_data %&gt;% dplyr::group_by(location) %&gt;% dplyr::filter(date == max(date)) %&gt;% dplyr::ungroup() There’s a problem here! For many countries the last day is filled with a NA, even though data is available for previous days. See, for example, the Netherlands. We know that 74.6% of its population was vaccinated with two doses by 2021-12-12. But there’s no data for the following seven days. vacc_data %&gt;% dplyr::filter(location == &#39;Netherlands&#39;) %&gt;% dplyr::slice_tail(n = 10) ## # A tibble: 10 × 3 ## location date people_fully_vaccinated_per_hundred ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Netherlands 2021-12-11 NA ## 2 Netherlands 2021-12-12 74.6 ## 3 Netherlands 2021-12-13 NA ## 4 Netherlands 2021-12-14 NA ## 5 Netherlands 2021-12-15 NA ## 6 Netherlands 2021-12-16 NA ## 7 Netherlands 2021-12-17 NA ## 8 Netherlands 2021-12-18 NA ## 9 Netherlands 2021-12-19 NA ## 10 Netherlands 2021-12-20 NA We could think of different ways to retain the most recent non-NA observation in each country. But since the percentage of people vaccinated cannot decrease, it’s entirely fair to carry this observation forward and consider it the most recent figure. How can we do it? The fill() function from the tidyr package is designed to fill missing values with the previous (default) or next non-NA value, the main argument being the columns we want to fill. At this point, I’d like to call your attention for another powerful tool available in dplyr 1.0.0 or higher. Remember that we used across to select variables using the pattern in their names? The selection helper where does something similar, but instead of a name pattern it selects the variables for which a given function returns TRUE. For example, we know that the variable we want to fill is numeric. So we can use this condition to prevent us from writing it. It won’t save us a lot of effort since we have only one variable with this condition here, but what if we had dozens? vacc_data_last_nonNA &lt;- vacc_data %&gt;% dplyr::group_by(location) %&gt;% dplyr::arrange(date) %&gt;% tidyr::fill(where(is.numeric)) %&gt;% dplyr::filter(date == max(date)) %&gt;% dplyr::ungroup() We can check what happened to Netherlands’ data for the last seven days. The last value from Dec 12th has been successfully carried forward up to Dec 19th. Now we’re ready to create our ranking and then plot it to get e better sense of the inequality in the vaccination race between countries. Notice I use n() inside the dplyr::slice() function. It’s a great shortcut that can be used inside dplyr functions to avoid writing the number of rows in a data frame (or group). Also, note the use of forcats::fct_reorder. The forcats package contains a lot of useful functions to handle factors and fct_reorder is the one I use most. It reorder factor levels by sorting along another variable. In this case, as we are making a rank, it would be great if the locations were ordered by their position in the rank! vacc_rank &lt;- vacc_data_last_nonNA %&gt;% dplyr::mutate(rank = dplyr::dense_rank(desc(people_fully_vaccinated_per_hundred))) %&gt;% dplyr::arrange(rank) %&gt;% dplyr::slice(c(1:10, n():(n()-9))) vacc_rank %&gt;% dplyr::mutate(rank_color = if_else(rank &lt;= 10, &#39;good&#39;, &#39;bad&#39;)) %&gt;% ggplot(aes(x = forcats::fct_reorder(location, desc(rank)), y = people_fully_vaccinated_per_hundred)) + geom_col(aes(fill = rank_color)) + scale_fill_manual(values = c(&#39;red&#39;, &#39;blue&#39;)) + scale_y_continuous(labels = function(x) glue::glue(&#39;{x}%&#39;)) + geom_text(aes(label = glue::glue(&#39;{people_fully_vaccinated_per_hundred}%&#39;)), nudge_y = 8) + theme_light() + theme(legend.position = &#39;none&#39;) + coord_flip() + labs(title = &#39;Percentage of population fully vaccinated against COVID-19&#39;, x = &#39;&#39;, y = &#39;&#39;) "]
]
