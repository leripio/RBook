# The basics of Tidyverse {#tidyverse}

```{r setup0, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

The purpose of this chapter is to introduce the main functions of the core packages in tidyverse. It's not intended to be a thorough description of every function, rather the idea is to provide the basic tools so that those who are either base R users or aren't R users at all can follow along with the rest of the book. For a more detailed approach to tidyverse, the best source is [R for Data Science](https://r4ds.had.co.nz/) by Hadley Wickham & Garrett Grolemund. For those who feel comfortable with tidyverse, a quick skim through the chapter might be enough.

## What's tidyverse?

To start, what's tidyverse? According to the [official website](https://www.tidyverse.org/):

> The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.

From my own experience I can ensure you that tidyverse is all you need to carry out data analyses as **neat**, **flexible** and **readable** as possible. By the way, I believe these are the three most important goals for those who want to thrive as a data scientist.

By **neat** I mean succinct code without any redundancy, a single block of code (not a bunch of loose objects) doing all the desired task. **Flexible**, in turn, means the code is general enough. It depends on the nature of the task at hand, so take it more like a philosophy. For example, does the solution still work if the data was updated with new values? Does it depend either on column names or the specific length of the vector? Finally, **readable** means that not only the understanding of the code is easy, but also that it's straightforward to make adjustments to it if necessary.

Most of the time writing neat, flexible and readable code takes longer and requires a dose of creativity, but it surely pays off. Moreover, with practice this will become more and more natural and the cost will get significantly lower.

In the next sections we'll review the most used functions to go through all the steps of everyday data analyses using core tidyverse packages designed to **Import**, **Wrangle**, **Program** and **Plot**.

## Import

### Reading from files

`readr` is the package used to import data from flat files (CSV and TXT). The most generic function is `read_delim` as it has a large set of arguments that allow us to declare the structure of the data we want to import. In practice, we often use the `read_csv` which is a special case of the former with some predefined settings suitable for CSV files -- the most obvious being the comma as the column delimiter.

If you're running RStudio IDE, I recommend you to click on **Import Dataset** at the **Environment** sheet (by default it's located in the upper right panel) to manually define the appropriate settings for your file. **We should never be encouraged to use windows, click on buttons or any kind of shortcuts provided by the IDE. This is the only exception I think is worth it**. First, because it can be tedious to set many specific parameters via trial-and-error to correctly import your data. Second, because once you have the configuration done the code is displayed at the bottom of the window so you can copy it and paste it on your script and, eventually, get rid of this shortcut as you learn how things work. Lastly, you'll usually import one single data set for each task and there's not much to improve on this process. Then, Yes, it's a big waste of time trying to understand each single argument.

Just for the sake of illustration, let's use `readr` to import the COVID data from Our World in Data we'll use in Chapter \@ref(owidCovid). It's a standard CSV file, so we can use `read_csv` without any additional parameter. We just need to provide the file path or URL:

```{r cha00_chunk1, eval = F}

data_url <- 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv'
covid_data_from_url   <- readr::read_csv(data_url)
covid_data_from_local <- readr::read_csv('data/owid-covid-data.csv')

```

Tidyverse also provides packages for reading other usual file types. For instance, we can use `readxl` to read MS Excel spreadsheets and `haven` to read data generated by popular statistical softwares (SPSS, Stata and SAS) in much the same way we did with `readr` for CSV files -- you can check them at the **Import Dataset** dropdown menu. So we won't waste time covering them. Rather, let's use the rest of this section to talk about something very important and that usually gets less attention in books.

### Reading from API

Although the use of these traditional file formats are still widespread, organizations are increasingly switching to API's to make data available to the public. This is a huge improvement since we're able to customize our demand and retrieve only the data we need for the specific task.

If you don't know what an API is, think of it as an interface to establish a connection to a database. However, instead of reading all the content from this database you can inform your **preferences** like what columns you want, the desired observations (time range if it's a time series) and so on. These preferences are embedded in a **request** and the right form to specify it is usually available in a documentation provided by the API maintainer.

For example, the Federal Reserve Bank of St. Louis maintains a huge repository with hundreds of thousands of economic time series and we can import them using an API. The information on how to build the request is available in the [documentation](https://fred.stlouisfed.org/docs/api/fred/), where we can see a link with instructions on how to **'Get the observations or data values for an economic data series'**. There, we find an example of a request for the US Real Gross National Product:

<https://api.stlouisfed.org/fred/series/observations?series_id=GNPCA&api_key=abcdefghijklmnopqrstuvwxyz123456&file_type=json>

We can break this request to see its parts in more detail:

1.  The static URL to access the **Observations** section in the API.

$$
\underbrace{\text{https://api.stlouisfed.org/fred/series/observations?}}_\text{Static: API URL}
$$

2.  The series ID which identifies what series we want.

$$
\underbrace{\text{series_id=GNPCA}}_\text{Param. #1: Series ID}
$$

3.  The API Key. We must create an account and require a personal key in order to retrieve data from the API. This one is an example for illustrative purposes only.

$$
\underbrace{\text{api_key=abcdefghijklmnopqrstuvwxyz123456}}_\text{Param. #2: API Key}
$$

4.  The file type for the output. There are several types, but I'd rather working with JSON.

$$
\underbrace{\text{file_type=json}}_\text{Param. #3: File Type}
$$

Note that all the parameters following the static part is separated by an **&**. Then, if we want to add any extra parameter it should be placed right after an **&**.

Suppose we want to read monthly data for the [US Consumer Price Index (CPI)](https://fred.stlouisfed.org/series/CPALTT01USM657N). The `series_id` is **CPALTT01USM657N**. Besides, we'd only like to read data as of January 2010. How can we do so? There's a parameter `observation_start` which defines the start of the observation period (YYYY-MM-DD format).

The code below creates a separate object for each part of the request. Then, we use the `glue` function from the homonymous package to merge the pieces into a single string adding the appropriate **&**. Of course, we could create the complete string all at once. But, creating separate objects makes it easier to find and edit values as well as to transform this task into a function if we had to read other data from this source frequently (more on this later in Chapter \@ref(google)).

```{r cha00_chunk2}

api_url       <- 'https://api.stlouisfed.org/fred/series/observations?'
api_key       <- 'bc0b91e1c6fa2cab4d4f180c7965edfa'
api_series_id <- 'CPALTT01USM657N'
obs_start     <- '2010-01-01'
api_filetype  <- 'json'
api_request   <- glue::glue('{api_url}series_id={api_series_id}&observation_start={obs_start}&api_key={api_key}&file_type={api_filetype}')

```

Now, we use the `httr` package to connect to the API, send the request and get the content. The other steps transform the content from JSON to a standard R object (a list) and then convert it to a tibble (the tidyverse's improved format for data frames). The element `observations` is specific for this API, so if we imported data from another source we'd have to check in which element the desired content would be stored.

```{r ch00_chunk3}
library(magrittr)
cpi_request <- httr::GET(url = api_request)
cpi_content <- httr::content(cpi_request, as = 'text')
cpi_list    <- jsonlite::fromJSON(cpi_content, flatten = FALSE)
cpi_tbl     <- cpi_list[['observations']] %>% tibble::as_tibble()
cpi_tbl
```

This concludes our section on how to import data. If you're struggling with other types of data, Tidyverse's official [website](https://www.tidyverse.org/packages/#import) provides a comprehensive list of all the supported file formats and the respective packages used to handle them.

## Wrangle

This is by far the most important section in this Chapter. The main goal here is to provide a general sense of how to get raw data ready to use. For this, we'll focus on the roles of the main functions inside the core packages rather than the idiosyncrasies and generalizations of each one. A deeper approach will be taken in the next chapters.

### Data manipulation

Let's use the COVID data we read in the last section. Starting with the `glimpse` function to have a grasp of the data, we can see some useful information such as the number of rows and columns, as well as columns' names and classes (whether they're character, double, etc).

```{r ch00_chunk04}

covid_data <- readr::read_csv('data/owid-covid-data.csv')
covid_data %>% 
  dplyr::glimpse()

```

We're usually not interested in all of these data, so the first couple of tasks we'd like to perform is to filter the relevant categories and select the desired columns. For example, we could be interested in analyzing new COVID cases (column `new_cases`) only in Brazil (rows equal to `Brazil` in column `location`). Furthermore, we'd like to get rid of duplicate rows if there's any.

One of the great contributions of tidyverse is to assign names to the functions according to the actions they perform -- many are admittedly SQL-inspired. `distinct` select only unique rows from a data set, thus dropping duplicates. `select` picks variables based on their names, whereas `filter` retains the rows which satisfy a given condition -- or a set of conditions.

Conditions are sentences which returns `TRUE` or `FALSE`. It's straightforward to think of conditions using logical operators, such as `==`, `>`, `<`, etc. Nevertheless, there's a bunch of expressions in R which return `TRUE` or `FALSE`. Moreover, we can always create our own condition to suit the work purpose. We'll see many examples throughout this book.

We can use the code below to generate the desired subset of the data:

```{r ch00_chunk05}

covid_data_01 <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::select(date, continent, location, new_cases) %>% 
  dplyr::filter(location == 'Brazil')

```

Next, we'd probably need to create additional columns. For instance, suppose that actual new cases in Brazil are much higher than reported because the country doesn't have enough tests and a conservative estimate points to a number of, at least, twice the official count. So we'd like to add a column which is twice the value of the original one representing our guess of the real situation. In addition, we'd also like to create a column to indicate the dominant strain at each period of time. We know that the Delta strain took over Gamma by the end of July 2021 and, then, Omicron took over Delta in the beginning of 2022.

`mutate` can be used to create new variables as a function of existing variables, but not necessarily. Also, it can be used to modify existing variables as new variables overwrites existing variables of the same name. `case_when` is another SQL-inspired function used inside `mutate` to create a new variable based on conditions. It's worth noting that it'll return `NA` if no condition is met. In this case, a useful workaround is to define an extra condition as `TRUE ~ value`, thus assigning a value to all the unmet conditions -- think of this as an **else** condition.

```{r ch00_chunk06}

covid_data_02 <- covid_data_01 %>% 
  dplyr::mutate(
    real_new_cases = new_cases*2,
    dominant_strain  = dplyr::case_when(
      date <= '2021-07-31'                        ~ 'Gamma',
      date >  '2021-07-31' & date <= '2021-12-31' ~ 'Delta',
      date >  '2021-12-31' & date <= '2022-02-01' ~ 'Omicron',
      TRUE                                        ~ "We don't know"
    )
  )
        
```

So far we've worked on a single group of data: new cases in Brazil. However, we usually have many categories to work on. We might be interested in analyzing new cases in all European countries. In this case, we'll need the `group_by` function, which allows us to perform any operation by group. `group_by` is often used in conjunction with `mutate` or `summarise` to create new data for each group. The latter uses aggregate functions (`mean`, `max`, `min`, etc) to produce a summary of the data.

For example, suppose we want to know which European country recorded the highest number of new covid cases in a single day by the end of 2021. This might be achieved by grouping the data by location and then using `summarise` with `max`. In addition, we can use `arrange` to sort the rows by value (we use `desc` to sort in descending order). Don't forget to `ungroup` data as soon as you no longer need to perform operations by group.

```{r ch00_chunk07, warning=TRUE}

covid_data_03 <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::filter(continent == 'Europe',
                date <= '2021-12-31') %>% 
  dplyr::group_by(location) %>% 
  dplyr::summarise(max_new_cases = max(new_cases)) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(desc(max_new_cases))
        
```

Note that some countries such as Spain, Portugal and France returned `NA`. This happened because they probably have any missing value. We could easily ignore it by passing the argument `na.rm = TRUE` to the `max` function. However, another problem would arise: countries with no data on new cases would return `-Inf`. To get around these two issues, we can filter out all the missing values in the data set (the `!` before the condition works as a negation). In this case removing the missing values isn't a problem, but be aware that for some tasks this may influence the outcome.

```{r ch00_chunk08, warning=TRUE}

covid_data_04 <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::filter(continent == 'Europe',
                date <= '2021-12-31',
                !is.na(new_cases)) %>% 
  dplyr::group_by(location) %>% 
  dplyr::summarise(max_new_cases = max(new_cases)) %>% 
  dplyr::ungroup()
   
covid_data_04     

```

In addition, we might want to know not only what the highest numbers were but when they did occur (the peak date). Since the `summarise` function is designed to return a single value, we must use an expression which returns a single value. If we used `date = max(date)`, we'd keep the most recent date for the data in each country. Definitely, that's not what we want. So a good way to address this issue is to combine a subset operation with a condition inside. In simpler terms, we'll subset from the column date the observation where new cases were at their high. Since we can have multiple dates which satisfy this condition, we'll keep the most recent one (the `max` of them).

```{r ch00_chunk09, warning=TRUE}

covid_data_05 <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::filter(continent == 'Europe',
                date <= '2021-12-31',
                !is.na(new_cases)) %>% 
  dplyr::group_by(location) %>% 
  dplyr::summarise(max_new_cases = max(new_cases),
                   peak_date = date[which(new_cases == max_new_cases)],
                   peak_date = max(peak_date)) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(peak_date) %>% 
  dplyr::slice(1:10)
   
covid_data_05   

# Verificar depois por que Vaticano aparece com 2 entradas

```

In case it went unnoticed, the previous code showed a really nice feature of `mutate`/`summarise`: you can use a variable you've just created in a subsequent task inside the same calling. In this example, we used the maximum number of new cases (first variable created) as a reference to obtain the dates when it occurred (second variable created) and them picked only the maximum date from a possibly set of dates (third variable created), all of this inside the same `summarise` calling as if all steps were performed sequentially. 

Finally, we used `arrange` to sort countries by their peak date (this time by ascending order, the default) and then `slice` to subset only the first ten countries out of the forty-nine in our data set.

**Falar dos joins, bind-rows/cols**

### Data layout

We've walked through all the main functions of `dplyr`. Now, we turn to the `tidyr` package. According to tidyverse's website, the goal of `tidyr` is to help us create **tidy data**. It means a data set where every column is a variable; every row is an observation; and each cell is a single value. This is more like a convention created by tidyverse so that *"you’ll spend less time fighting with the tools and more time working on your analysis"*.

Tidy data is also known as **wide** format -- since it increases the number of columns and decreases the number of rows --, as opposed to the **long** format, where data are stacked increasing the number of rows and decreasing the number of columns. It took me a while before I could tell if a data set was in wide or long format, so don't worry if it's not so clear right now. Perhaps a more direct way of thinking about this distinction is to ask yourself: *Is all the information contained in a single cell?*. If yes, it's wide format. If not, it's long format. 

For example, is the covid data set in wide or long format? If we took a single cell from the **new_cases** column, does it convey all the information for this variable? No, it doesn't. We know the number of new cases in a given date, but we don't know the country that value refers to -- is this from Germany? Nigeria? Chile? 

We can use `tidyr::pivot_wider` to convert from long to wide format. The syntax is very easy to understand: `names_from` is the column we want to widen, whereas `values_from` is the column containing the observations that will fill each cell. `id_cols` is an optional argument used to declare the set of columns that uniquely identifies each observation. In practice, it drops all the other columns in the data set. Hence, if we want to keep all the other variables, just skip it.     

```{r ch00_chunk10}

covid_wide_01 <- covid_data %>% 
  tidyr::pivot_wider(id_cols     = c('date', 'location'), 
                     names_from  = 'location', 
                     values_from = 'new_cases')

covid_wide_01

```

Notice that now we have 239 columns rather than 67 of the original data set with each cell conveying the whole information: new cases for a given date for a specific country. So, what if we wanted to have both `new_cases` and `new_deaths` columns in wide form? We just need to provide a vector with the desired variables in `values_from`. By default, the new columns will be named according to the following pattern: **variable_location**.

Since the variables names already contain an underscore, it's a good idea to set a different character as separator. This is because we might need to reverse the operation later for a given task, then it's much easier to identify it. Otherwise, we'd have to use regular expression to inform the specific position of the repeated character -- for example, whether it's the first or the second underscore. 

```{r ch00_chunk11}

covid_wide_02 <- covid_data %>% 
  tidyr::pivot_wider(id_cols     = c('date', 'location'), 
                     names_from  = 'location', 
                     values_from = c('new_cases', 'new_deaths'),
                     names_sep   = '-')

covid_wide_02

```

Our new data set expanded to 477 columns. As we create more and more columns to arrange more variables it becomes harder to perform some simple tasks. For example, it's much easier to use `group_by` with `mutate` or `summarise` when we have the data set in long format. In addition, using `filter` is generally easier than using a conditional `select` when we want to keep only the relevant data. 

In summary, long format may be preferable over wide format when the data set contains more than one grouping variable or we want to work on more than one variable. Besides, long format data sets are ideal for plotting with `ggplot2` package as we'll see later. Therefore, it's not unusual to convert a data set from wide to long format. 

The syntax is very similar to what we saw earlier when converting from long to wide format. The unique difference is in `cols` argument, used to declare what columns we want to stack. However, since wide data sets usually have a large number of columns and we're often interested in putting all of them in long format, it's much easier to declare what columns we want to leave out (`-`). 

```{r ch00_chunk12}

covid_long_01 <- covid_wide_02 %>% 
  tidyr::pivot_longer(cols      = -'date',
                      names_to  = c('variable', 'location'),
                      values_to = 'value',
                      names_sep = '-')

covid_long_01

```

Note that now even the variables (new_cases and new_deaths) are stored in a single column (variable). This is probably an abuse of language, but I call this a complete long format -- as opposed to the original format where the data set was only partially in the long format (the variables were indeed in wide format). For most applications, I think this is the best way to organize the data. 

Converting a data set between wide and long formats might not completely solve our problem. Sometimes, two pieces of information are merged in a single column. For example, suppose that the `location` column had both the continent and country names instead of only the country as in the original data set. We'll call this data set `covid_loc_cont`

```{r ch00_chunck13, echo = F}

covid_loc_cont <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::filter(!is.na(location), !is.na(continent)) %>% 
  tidyr::unite(c('location', 'continent'), col = 'location', sep = '_') %>% 
  dplyr::select(location, date, new_cases, new_deaths)

covid_loc_cont

```

This is undesirable since we can no longer use `group by` or `filter` over continents alone, for instance. Hence, the best practice is to have a single column for each variable. This can be easily achieved using the `separate` function, with a highly self-explanatory syntax. Again, the separator character being unique in the string makes the job much easier.

```{r ch00_chunck14}

covid_separate <- covid_loc_cont %>% 
  dplyr::distinct() %>% 
  tidyr::separate(col  = 'location',
                  into = c('location', 'continent'),
                  sep  = '_')

covid_separate

```

The only caveat to all this simplicity is when we have non-trivial separators. For example, imagine that we had no underscore to separate location from continent.   

```{r ch00_chunck15, echo = F}

covid_loc_cont2 <- covid_data %>% 
  dplyr::distinct() %>% 
   dplyr::filter(!is.na(location), !is.na(continent)) %>% 
  tidyr::unite(c('location', 'continent'), col = 'location', sep = '') %>% 
  dplyr::select(location, date, new_cases, new_deaths)

covid_loc_cont2

```

How could we manage to separate them? Ideally, we should provide a regular expression (or simply regex) to match the appropriate pattern to split the string into location and continent (the `sep` argument works with regex). If you don't know what regex is, think of it as a code used to match patterns, positions and all kinds of features in a string.   

At first glance, a natural choice would be to split the string as of the second uppercase letter. This would work for Afghanistan, France, Netherlands, Chile and all single-worded countries. However, this would fail for countries with two or more words: United States, New Zealand and many others. 

Then, you could argue, a more general approach would be to use regex to match the last uppercase letter in the string. Not actually, because we have a couple of two-worded continents: North America and South America. So, for example, **CanadaNorth America** would be split into **CanadaNorth** and **America** instead of **Canada** and **North America**.  

More often than not, the direct solution is the most difficult to implement (or we simply don't know how to accomplish it) and so we have to think about alternative ways. Data science is all about this. Since a regex solution alone might be very tough or even unfeasible, we'll get back to this problem later when covering text manipulation with the `stringr` package.      

For now, let's just get done with the `tidyr` package looking at two more very commonly used functions. The first one is `unite`, which is the counterpart of `separate`. We can use it to convert the `covid_separate` data frame back to the original form as in `covid_loc_cont`.

```{r ch00_chunk16}

covid_unite <- covid_separate %>% 
  tidyr::unite(c('location', 'continent'), col = 'location', sep = '_')

covid_unite

```

Last but not least is the `nest` function. The `nest` function is usually used in conjunction with `dplyr::group_by` in order to create a new format of data frame in which every cell is now a list rather than a single observation and, thus, might store any kind of object -- data frames, lists, models, plots and so forth. 

The example below shows how to create a nested data frame for the covid data grouped by country. Note that each cell on column `data` is now a data frame itself corresponding to the data for each country.

```{r ch00_chunk17}

covid_eu_nest <- covid_data %>% 
  dplyr::group_by(location) %>% 
  tidyr::nest()

covid_eu_nest

```

I find it most useful when we need to perform several tasks over whole datasets. For example, if we had to transform the raw data, create graphics and fit models for each country in the covid dataset. The base R solution in this case would be to create list of lists, which can be very confusing sometimes. We'll come back to nested data frames when we talk about functional programming with the `purrr` package.

### Text manipulation

The need to handle text data has grown substantially in parallel with the popularity of Machine Learning models and, more specifically, Natural Language Processing (NLP). For those more advanced applications, the challenge is to standardize a large set of (messy) texts in order to extract features which can then feed the models and generate predictions. 

Nevertheless, knowing the basics of text manipulation is critical for everyday tasks. It includes subsetting parts of a word, detecting if specific patterns are present, replacing a sequence of characters by something else and so forth. The functions from the `stringr` package do a terrific job in simplifying all these operations and go far beyond. 

Similarly to what we did in the previous sections, we'll focus on the most widely used functions. Let's start with `str_detect`, which is conveniently used in conjunction with `dplyr::filter` since it returns `TRUE` if the specific pattern is found in the string or `FALSE` otherwise.

For example, let's say we want to analyze the covid data only for North and South Americas. We've learned how to do so using `dplyr::filter`.

```{r ch00_chunk18}

covid_americas <- covid_data %>% 
  dplyr::filter(continent %in% c('North America', 'South America')) 
```

This one is not cumbersome. But let's pretend there are, say, 50 continents on Earth with 25 having 'America' in their names. Would it still make sense to write all these 25 names in a vector? Clearly not. Since all of them share a commom pattern, we can easily employ `str_detect` to do the trick:

```{r ch00_chunk19}

covid_americas2 <- covid_data %>% 
  dplyr::filter(stringr::str_detect(continent, 'America'))

```

Note that we could have left aside several characters using only, say, 'Am' or 'Ame'. It would work fine if there is no other continent with this sequence of characters. Of course, this parsimony makes more sense for lengthy words or long sentences. For short words it's recommended to write the full word in order to prevent any undesirable output. 

In addition, we can also provide multiple patterns to `str_detect` by separating them with a `|`. 

```{r ch00_chunk20}

covid_americas3 <- covid_data %>% 
  dplyr::filter(stringr::str_detect(continent, 'South|North'))

```

Finally, we may use the `negate = TRUE` argument if we're interested in the opposite of the pattern we provide -- pretty much like `!` in conditions. It's specially useful when we want to keep most categories but one (or a few). For example, suppose that now we want to analyze every continent except for Asia. Instead of writing them all, we could simply do the following:

```{r ch00_chunk21}

covid_exAsia <- covid_data %>% 
  dplyr::filter(stringr::str_detect(continent, 'Asia', negate = TRUE))

```

Another recurrent task when we're dealing with strings is to remove a part of it. It's generally required in order to establish a a standard within categories so we can perform further operations. The complexity of this task varies depending on the form of this undesired part. 

The most simple case is when we have to remove a sequence of characters which is fixed both in length and in position. For example, suppose we have a data frame `covid_numCont` in which the observations in the continent column starts with a random number from 0 to 9 -- think of it as a typing error from the source, since the same reasoning applies if those random numbers were present only in a few observations.

```{r ch00_chunk22, echo = F}

set.seed(123)

covid_numCont <- covid_data %>% 
  dplyr::filter(!is.na(continent)) %>% 
  dplyr::mutate(continent = sapply(continent, function(x) paste(sample(1:9, 1), x, sep = '.'))) %>% 
  dplyr::select(2:4, 6)


covid_numCont %>% 
  dplyr::slice_sample(n =  10)

```

To solve this is solely a matter of subsetting the string from position 3 onwards using `str_sub`. The `end` argument defaults to last character, so we don't need to explicit it. 

```{r ch00_chunk23}

covid_numCont %>% 
  dplyr::mutate(continent = stringr::str_sub(continent, start = 3))

```

Nice. But what if the random numbers ranged from 0 to 10? With an extra digit, we could no longer resort to the previous solution. 

```{r ch00_chunk24, echo = F}

set.seed(123)

covid_numCont <- covid_data %>% 
  dplyr::filter(!is.na(continent)) %>% 
  dplyr::mutate(continent = sapply(continent, function(x) paste(sample(1:10, 1, prob = c(rep(0.3, 9), 0.6)), x, sep = '.'))) %>% 
  dplyr::select(2:4, 6)


covid_numCont %>% 
  dplyr::slice_sample(n =  10)

```

We won't dedicate an exclusive section to regular expressions, but simple examples will eventually show up throughout the book. In this case, we could use a simple regular expression inside `str_remove` to get rid of everything before and up to the `.` -- `.*\\.`. 

```{r ch00_chunk25}

covid_numCont %>% 
  dplyr::mutate(continent = stringr::str_remove(continent, ".*\\."))

```

Typing errors may arise under different forms along the character column. These cases usually require a more thorough evaluation and, more often than not, the solution is to manually replace the wrong words by the correct ones.

For instance, in the data frame below (named `covid_typo`) we can find two different typos for North America: there's a missing **h** in positions 1 and 5; while there's an extra **h** in positions 3 and 8. Since the data frame contains a huge number of observations, it may occur in other positions as well. We can use `str_replace` to fix it for the whole column. 

```{r ch00_chunk26, echo = F}

set.seed(123)

covid_typo <- covid_data %>% 
  dplyr::filter(continent == 'North America') %>% 
  dplyr::select(2:4, 6)

covid_typo[c(1,5), ]$continent <- 'Nort America'
covid_typo[c(3,8), ]$continent <- 'Northh America'

covid_typo

```

```{r ch00_chunk27, echo = F}

set.seed(123)

covid_typo <- covid_data %>% 
  dplyr::filter(continent == 'North America') %>% 
  dplyr::select(2:4, 6)

covid_typo[c(1,5), ]$continent <- 'Nort America'
covid_typo[c(3,8), ]$continent <- 'Northh America'

covid_typo %>% 
  dplyr::mutate(
    continent = stringr::str_replace(continent, 'Nort America', 'North America'),
    continent = stringr::str_replace(continent, 'Northh America', 'North America')
    )

```

```{r ch00_chunk28, eval = FALSE}

covid_typo %>% 
  dplyr::mutate(
    continent = stringr::str_replace(continent, 'Nort America', 'North America'),
    continent = stringr::str_replace(continent, 'Northh America', 'North America')
    )

```

Or we can simply pass a vector with all the replacements to the `str_replace_all` function.

```{r ch00_chunk29}

covid_typo %>% 
  dplyr::mutate(
    continent = stringr::str_replace_all(continent, c('Nort America'   = 'North America',
                                                      'Northh America' = 'North America')
                                         )
    )

```

There's one last kind of typo we can't help but to mention: whitespace. Whitespaces are particularly troublesome when they're misplaced in the start/end of string or repeated inside it. Because they're very easy to miss the `stringr` package contains two functions to cope with them: `str_trim` and `str_squish`. The former removes whitespaces from start/end of string, whereas the later removes them inside a string. 

The data frame below (named `covid_ws`) uses the same example as above, but now with a whitespace in the end of observations 1 and 5; and a repeated whitespace inside the observations 3 and 8. 

```{r ch00_chunk30, echo = F}

set.seed(123)

covid_ws <- covid_data %>% 
  dplyr::filter(continent == 'North America') %>% 
  dplyr::select(2:4, 6)

covid_ws[c(1,5), ]$continent <- 'North America '
covid_ws[c(3,8), ]$continent <- 'North  America'

covid_ws

```

We can easily get rid of them using those functions. It's advisable to use them as a preprocessing step whenever we're working with character columns that should not have these extra whitespaces.

```{r ch00_chunk32, eval = FALSE}

covid_ws %>% 
  dplyr::mutate(
    continent = stringr::str_trim(continent),
    continent = stringr::str_squish(continent)
  )

```

```{r ch00_chunk31, echo = F}

set.seed(123)

covid_ws <- covid_data %>% 
  dplyr::filter(continent == 'North America') %>% 
  dplyr::select(2:4, 6)

covid_ws[c(1,5), ]$continent <- 'North America ' 
covid_ws[c(3,8), ]$continent <- 'North  America'

covid_ws %>% 
  dplyr::mutate(
    continent = stringr::str_trim(continent),
    continent = stringr::str_squish(continent)
  )

```

To finish up, let's use the tools we've just learned to solve that problem in Section X. 


### Date manipulation


Falar stringr, purrr, ggpplot.

Uma seção dedicada a missing values: como visualizar e preencher?