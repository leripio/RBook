# The basics of Tidyverse {#tidyverse}

```{r setup0, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
Sys.setlocale("LC_TIME", "en_US.utf-8")
```

The purpose of this chapter is to introduce the main functions of the core packages in tidyverse. It's not intended to be a thorough description of every function, rather the idea is to provide the basic tools so that those who are either base R users or aren't R users at all can follow along with the rest of the book. For a more detailed approach to tidyverse, the best source is [R for Data Science](https://r4ds.had.co.nz/) by Hadley Wickham & Garrett Grolemund. For those who feel comfortable with tidyverse, a quick skim through the chapter might be enough.

## What's tidyverse?

To start, what's tidyverse? According to the [official website](https://www.tidyverse.org/):

> The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.

From my own experience I can ensure you that tidyverse is all you need to carry out data analyses as **neat**, **flexible** and **readable** as possible. By the way, I believe these are the three most important goals for those who want to thrive as a data scientist.

By **neat** I mean succinct code without any redundancy, a single block of code (not a bunch of loose objects) doing all the desired task. **Flexible**, in turn, means the code is general enough. It depends on the nature of the task at hand, so take it more like a philosophy. For example, does the solution still work if the data is updated with new values? Or does it depend either on column names or the specific length of the vector? Finally, **readable** means that not only the understanding of the code is easy, but also that it's straightforward to make adjustments to it if necessary.

Most of the time writing neat, flexible and readable code takes longer and requires a dose of creativity, but it surely pays off. Moreover, with practice this will become more and more natural and the cost will get significantly lower.

In the next sections we'll review the most used functions to go through all the steps of everyday data analyses using core tidyverse packages designed to **Import**, **Wrangle**, **Program** and **Plot**.

## Importing {#import}

### Reading from flat files

`readr` is the tidyverse's package used to import data from flat files (CSV and TXT). The most generic function is `read_delim` as it has a large set of parameters that allow us to declare the structure of the data we want to import. In practice, we often use the `read_csv` which is a special case of the former with some predefined settings suitable for CSV files -- the most obvious being the comma as the column delimiter.

If you're running RStudio IDE, I recommend you to click on **Import Dataset** at the **Environment** sheet (by default it's located in the upper right panel) to manually define the appropriate settings for your file. **We should never be encouraged to use windows, click on buttons or any kind of shortcuts provided by the IDE. This is the only exception I think is worth it**. First, because it can be tedious to set many specific parameters via trial-and-error to correctly import your data. Second, because once you have the configuration done the code is displayed at the bottom of the window so you can copy it and paste it on your script and, eventually, get rid of this shortcut as you learn how things work. Lastly, you'll usually import one single data set for each task and there's not much to improve on this process. Then, Yes, it's a big waste of time trying to understand each single parameter.

Just for the sake of illustration, let's use `readr` to import the COVID data set from Our World in Data we'll use in Chapter \@ref(owidCovid). It's a standard CSV file, so we can use `read_csv` without any additional parameter. We just need to provide the file path or URL:

```{r cha00_chunk1, eval = T}

data_url <- 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv'
covid_data_from_url   <- readr::read_csv(data_url)
covid_data_from_local <- readr::read_csv('data/owid-covid-data.csv')

```

Tidyverse also provides packages for reading other usual file types. For instance, we can use `readxl` to read MS Excel spreadsheets and `haven` to read data generated by popular statistical softwares (SPSS, Stata and SAS) in much the same way we did with `readr` for CSV files (you can check them at the **Import Dataset** dropdown menu) so we won't waste time covering them. Rather, let's use the rest of this section to talk about something very important and that usually gets less attention in books: reading data from an API.

### Reading from API {#readapi}

Although the use of traditional flat files are still widespread, organizations are increasingly switching to API to make data available to the public. This is a huge improvement since we're able to customize our demand and retrieve only the data we need for the specific task.

If you don't know what an API is, think of it as an interface to establish a connection to a database. However, instead of reading all the content from this database you can inform your **preferences** like what columns you want, the desired observations (time range if it's a time series) and so on. These preferences are embedded in a **request** and the right form to specify it is usually available in a documentation provided by the API maintainer.

For example, the Federal Reserve Bank of St. Louis maintains a huge repository with hundreds of thousands of economic time series and we can import them using an API. The information on how to build the request is available in the [documentation](https://fred.stlouisfed.org/docs/api/fred/), where we can see a link with instructions on how to **'Get the observations or data values for an economic data series'**. There, we find an example of a request for the US Real Gross National Product:

<https://api.stlouisfed.org/fred/series/observations?series_id=GNPCA&api_key=abcdefghijklmnopqrstuvwxyz123456&file_type=json>

We can break this request down to see its parts in more detail:

1.  The static URL to access the **Observations** section in the API.

$$
\underbrace{\text{https://api.stlouisfed.org/fred/series/observations?}}_\text{Static: API URL}
$$

2.  The series ID which identifies the data for the US Real Gross National Product.

$$
\underbrace{\text{series_id=GNPCA}}_\text{Param. #1: Series ID}
$$

3.  The API Key. We must create an account and require a personal key in order to retrieve data from the API. This one is an example for illustrative purposes only.

$$
\underbrace{\text{api_key=abcdefghijklmnopqrstuvwxyz123456}}_\text{Param. #2: API Key}
$$

4.  The file type for the output. There are several types, but I'd rather working with JSON.

$$
\underbrace{\text{file_type=json}}_\text{Param. #3: File Type}
$$

Note that all the parameters following the static part is separated by an **&**. Then, if we want to add any extra parameter it should be placed right after this symbol.

Suppose we want to read monthly data for the [US Consumer Price Index (CPI)](https://fred.stlouisfed.org/series/CPALTT01USM657N). The `series_id` is **CPALTT01USM657N**. Besides, we'd only like to read data as of January 2010. How can we do so? There's a parameter `observation_start` which sets the start of the observation period (YYYY-MM-DD format).

The code below creates a separate object for each part of the request. Then, we use the `glue` function from the homonymous package to merge the pieces into a single string adding the **&** symbol between parameters. We could create the full string all at once,  but creating separate objects makes it easier to find and edit values as well as to transform this task into a function if we had to frequently read data from this source (more on this later).

```{r cha00_chunk2}

api_url       <- 'https://api.stlouisfed.org/fred/series/observations?'
api_key       <- 'bc0b91e1c6fa2cab4d4f180c7965edfa'
api_series_id <- 'CPALTT01USM657N'
obs_start     <- '2010-01-01'
api_filetype  <- 'json'
api_request   <- glue::glue('{api_url}series_id={api_series_id}&observation_start={obs_start}&api_key={api_key}&file_type={api_filetype}')

```

Now we use the `httr` package to connect to the API, send the request and get the content. The other steps transform the content from JSON to a standard R object (a list) and then convert it to a tibble (the tidyverse's improved format for data frames). Notice that the CPI data is stored in the list element named `observations`. However, this is specific for this API and if we were to import data from another source we'd have to check in which element the data would be stored.

```{r ch00_chunk3}
library(magrittr)
cpi_request <- httr::GET(url = api_request)
cpi_content <- httr::content(cpi_request, as = 'text')
cpi_list    <- jsonlite::fromJSON(cpi_content, flatten = FALSE)
cpi_tbl     <- cpi_list[['observations']] %>% tibble::as_tibble()
cpi_tbl
```

This concludes our section on how to import data. If you're struggling with other types of data, Tidyverse's official [website](https://www.tidyverse.org/packages/#import) provides a comprehensive list of all the supported file formats and the respective packages used to handle them.

## Wrangling {#wrangling}

This is by far the most important section in this Chapter. The main goal here is to provide a general sense of how to get raw data ready to use. For this, we'll focus on the roles of the main functions from `dplyr` package rather than the idiosyncrasies and generalizations of each one. Best practices will be shown in the next chapters.

### Data manipulation {#wrangling-data-manipulation}

Let's use the COVID data we read in the last section. Starting with the `glimpse` function to have a grasp of the data, we can see some useful information such as the number of rows and columns, as well as columns' names and classes (whether they're character, double, etc).

```{r ch00_chunk04}

library(tidyverse)
covid_data <- readr::read_csv('data/owid-covid-data.csv')
covid_data %>% 
  dplyr::glimpse()

```

We're usually not interested in all of these data, so the first couple of tasks we'd like to perform is to filter the relevant categories and select the desired columns. For example, we could be interested in analyzing new COVID cases (column `new_cases`) only in Brazil (rows equal to `Brazil` in column `location`). Furthermore, we'd like to get rid of duplicate rows if there's any.

One of the great contributions of tidyverse is to assign names (verbs) to the functions according to the actions they perform -- many are admittedly SQL-inspired. For example, `distinct` drops observations (rows) which are not unique, whereas `select` picks variables based on their names. The exception is `filter`, which retains the rows which satisfy a given condition (the analogue of WHERE in SQL).

Conditions are sentences which returns `TRUE` or `FALSE`. It's straightforward to think of conditions using logical operators, such as `==`, `>`, `<`, etc. Nevertheless, there's a bunch of expressions in R which return `TRUE` or `FALSE`. Moreover, we can always create our own condition to generate the desired output. We'll see many examples throughout this book.

The code below performs the initial steps described above to generate a subset of the data.

```{r ch00_chunk05}

covid_data_01 <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::select(date, continent, location, new_cases) %>% 
  dplyr::filter(location == 'Brazil')

```

Next, we'd probably need to create additional columns. For instance, suppose that actual new cases in Brazil are much higher than reported because the country doesn't have enough tests and a conservative estimate points to a number of, say, twice the official count. So we'd like to add a column which is twice the value of the original one representing our guess of the real situation. In addition, we'd also like to create a column to indicate the dominant strain at each period of time. We know that the Delta strain took over Gamma by the end of July 2021 and, then, Omicron took over Delta in the beginning of 2022.

The `mutate` verb can be used to create new variables as a function of existing ones. Also, it can be used to modify existing variables as new variables overwrites those with the same name. `case_when` is another SQL-inspired function used inside `mutate` to create a new variable based on conditions. It's worth noting that it returns `NA` if no condition is met. In this case, a useful workaround is to define an extra condition as `TRUE ~ value`, thus assigning a value to all the unmet conditions -- think of this as an **else** condition.

```{r ch00_chunk06}

covid_data_02 <- covid_data_01 %>% 
  dplyr::mutate(
    real_new_cases = 2*new_cases,
    dominant_strain  = dplyr::case_when(
      date <= '2021-07-31'                        ~ 'Gamma',
      date >  '2021-07-31' & date <= '2021-12-31' ~ 'Delta',
      date >  '2021-12-31' & date <= '2022-02-01' ~ 'Omicron',
      TRUE                                        ~ "We don't know"
    )
  )
        
```

The `between` function is a shortcut for numeric conditions that are bounded both on the left and on the right. It also works with dates if we declare the arguments as date objects. Therefore, we can replace conditions 2 and 3 to have a more compact and efficient code (it's implemented in C++).

```{r ch00_chunk062}

covid_data_02 <- covid_data_01 %>% 
  dplyr::mutate(
    real_new_cases = 2*new_cases,
    dominant_strain  = dplyr::case_when(
      date <= '2021-07-31'                                          ~ 'Gamma',
      between(date, as.Date('2021-07-31'), as.Date('2021-12-31'))   ~ 'Delta',
      between(date, as.Date('2021-12-31'), as.Date('2022-02-01'))   ~ 'Omicron',
      TRUE                                                          ~ "We don't know"
    )
  )
        
```

So far we've worked on a single group of data: new cases in Brazil. However, we usually have many categories to work on. We might be interested in analyzing new cases in all European countries, for example. In this case, we'll need the `group_by` function, which allows us to perform operations by group. `group_by` is often used in conjunction with `mutate` or `summarise` to create new data for each group. The latter uses aggregate functions (`mean`, `max`, `min`, etc) to produce a summary of the data.

For example, suppose we want to know which European country recorded the highest number of new covid cases in a single day by the end of 2021. This might be achieved by grouping the data by location and then using `summarise` with `max`. In addition, we can use `arrange` to sort the rows by value (we use `desc` to sort in descending order). Don't forget to `ungroup` data as soon as you no longer need to perform operations by group.

```{r ch00_chunk07, warning=TRUE}

covid_data_03 <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::filter(continent == 'Europe',
                date <= '2021-12-31') %>% 
  dplyr::group_by(location) %>% 
  dplyr::summarise(max_new_cases = max(new_cases)) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(desc(max_new_cases))
        
```

Note that some countries such as Spain, Portugal and France returned `NA`. This happened because they probably have any missing value. We could easily ignore it by passing the argument `na.rm = TRUE` to the `max` function. However, another problem would arise: countries with no data on new cases would return `-Inf`. To get around these two issues, we can filter all the missing values out in the data set using the logical operator `is.na()` (the `!` before the condition works as a negation). In this case removing the missing values isn't a problem, but be aware that for some tasks this may influence the outcome.

```{r ch00_chunk08, warning=TRUE}

covid_data_04 <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::filter(continent == 'Europe',
                date <= '2021-12-31',
                !is.na(new_cases)) %>% 
  dplyr::group_by(location) %>% 
  dplyr::summarise(max_new_cases = max(new_cases)) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(desc(max_new_cases))
   
covid_data_04     

```

In addition, we might want to know not only what the highest numbers were but when they did occur (the peak date). Since the `summarise` function is designed to return a single value, we must use an expression which returns a single value. If we used `date = max(date)`, we'd keep the most recent date for the data in each country. Definitely, that's not what we want. So a good way to address this issue is to combine a subset operation with a condition inside. In simpler terms, we'll subset from the column date the observation where new cases were at its high. Since we can have multiple dates which satisfy this condition, we'll keep the most recent one (the `max` of them).

```{r ch00_chunk09, warning=TRUE}

covid_data_05 <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::filter(continent == 'Europe',
                date <= '2021-12-31',
                !is.na(new_cases)) %>% 
  dplyr::group_by(location) %>% 
  dplyr::summarise(max_new_cases = max(new_cases),
                   peak_date = date[which(new_cases == max_new_cases)] %>% max()) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(desc(max_new_cases), peak_date) %>% 
  dplyr::slice(1:10)
   
covid_data_05   

```

In case it went unnoticed, the previous code showed a really nice feature of `mutate`/`summarise`: you can use a variable you've just created in a subsequent task inside the same calling. In this example, we used the maximum number of new cases (first variable) as a reference to obtain the dates when it occurred (second variable), all of this inside the same `summarise` calling as if all steps were performed sequentially. 

In the last two lines, we used `arrange` to sort countries firstly by the maximum number of new cases and then by their peak date and `slice` to subset only the first ten countries out of the forty-nine in our data set. This ends our approach to single data sets for now. 

But before we jump to the next section, there's something very important to address: merging multiple data sets. There are two families of functions in `dplyr` to merge data frames, `*_join` and `bind_*`. Let's see how they work.

The `*_join` functions are used to merge two data frames horizontally, matching their rows based on specified keys. For example, take the `covid_data_05` data frame we created above. It contains the top ten European countries with the highest number of new Covid cases in a single day and their peak dates. Suppose we want to add information on the population size for each country, which is available in another data frame named `europe_population` displayed below. 

```{r ch_00_chunk091, echo = FALSE}

europe_population <- covid_data %>% 
  dplyr::filter(continent == 'Europe') %>% 
  dplyr::select(Country = location, Pop_size = population) %>% 
  dplyr::distinct() %>% 
  dplyr::arrange(desc(Pop_size))

europe_population

```

What we want here is to add the column *Pop_Size* from `europe_population` into `covid_data_05` matching rows based on `location` (the column with countries in the main data set). For this, we can use the `left_join` function, which adds the content from the second data frame to the first one.     

```{r ch_00_chunk092}

covid_data_05_with_pop <- covid_data_05 %>% 
  dplyr::left_join(europe_population, by = c('location' = 'Country'))

covid_data_05_with_pop

```

We could have been assigned a slightly different task. For example, adding to `european_population` the information on maximum daily number of new Covid cases and peak dates we have from `covid_data_05`. This can also be achieved with `left_join` by reversing the order of the data frames (and names of the key column since they're not the same). 

An effortless alternative is to replace `left_join` with `right_join`, which adds to the second data frame (the one on the right) the information from the first data frame (the one on the left). In this case we don't need to change the order of the parameters. 

```{r ch_00_chunk093}

pop_with_covid_info <- covid_data_05 %>% 
  dplyr::right_join(europe_population, by = c('location' = 'Country'))

pop_with_covid_info

```

At first glance it seems to produce the same outcome, but notice that the number of rows are different in the two resulting data sets. In the first case `left_join` kept all the rows from `covid_data_05` and only the *corresponding* rows from `europe_population`, whereas `right_join` did the opposite. In summary, `left_join` and `right_join` keep all the rows from just one of the two data frames. 

Sometimes, however, we want to keep all the rows from both data frames. For example, imagine that `covid_data_05` had data not only on European but on South American countries as well. In addition, the `european_population` data frame also included the populations from Asian countries. If we merged the data frames using the functions above, we'd end up loosing observations on either Covid in South America or populations in Asia. 

In order to merge the data frames by their common countries while keeping the remaining observations from both, we should employ the `full_join` function. On the other hand, if we're willing to keep only the common countries in both data frames, this is the case for the `inner_join` function. 

These four functions comprise what it's called **mutating-joins** since they add columns from one data frame to the other. There are also the **filtering-joins** functions, which filter the rows from one data frame based on the presence (`semi_join`) or absence (`anti_join`) of matches in the other data frame. Given the latter category is used to a much lesser extent, I won't go into detail right now. Eventually they'll show up in the coming chapters.       

We saw that `*_join*` operations are particularly useful to merge data frames horizontally and by their matching rows according to key variables. More often than not we need to stack data vertically by their matching columns. For example, suppose the Covid data set was released as a single file for each country and we needed to perform some comparisons between France and the United Kingdom - `covid_fr` and `covid_uk` data sets shown below.

```{r ch00_chunk093, echo = FALSE}

covid_uk <- covid_data %>% 
  dplyr::filter(location == 'United Kingdom') %>% 
  dplyr::select(date, location, new_cases)

covid_fr <- covid_data %>% 
  dplyr::filter(location == 'France') %>% 
  dplyr::select(date, location, new_cases, total_cases)

covid_uk
covid_fr

```

This can be easily accomplished by `bind_rows` which, unlike the `*_join` family, allows us to provide several data frames. What's cool about `bind_rows` is that non-matching columns are kept with their values filled with `NA` for the data frames where the column is absent. In the example above, the `covid_fr` data set has a total cases column which is absent in `covid_uk`. 

```{r ch00_chunk094, echo = FALSE}

covid_fr_uk <- dplyr::bind_rows(covid_fr, covid_uk)
covid_fr_uk

```

The `bind_cols` function is more restrictive in this regard. It's used to merge the columns of data frames, but matching by row position rather than a key variable. For this reason, we can't have data frames of different lengths. In practice, it's much more common to rely on `*_join*` functions when we need to merge data frames horizontally.  

### Data layout {#datalay}

We've walked through the main functions of `dplyr`. Now, we turn to the `tidyr` package. According to tidyverse's website, the goal of `tidyr` is to help us create **tidy data**. It means a data set where every column is a variable; every row is an observation; and each cell is a single value. This is more like a convention created by tidyverse so that *"youâ€™ll spend less time fighting with the tools and more time working on your analysis"*.

Tidy data is also known as **wide** format -- since it increases the number of columns and decreases the number of rows --, as opposed to the **long** format, where data are stacked increasing the number of rows and decreasing the number of columns. It took me a while before I could tell if a data set was either in wide or long format, so don't worry if it's not so clear right now. Perhaps a more direct way of thinking about this distinction is to ask yourself: *Is all the information contained in a single cell?*. If so, it's wide format. If not, then it's long format. 

For example, is the covid data set in wide or long format? If we took a single cell from the **new_cases** column, does it convey all the information for this variable? No, it doesn't. We know the number of new cases in a given date, but we don't know the country that value refers to -- is this from Germany? Nigeria? Chile? 

We can use `tidyr::pivot_wider` to convert from long to wide format. The syntax is very easy to understand: `names_from` is the column we want to widen, whereas `values_from` is the column containing the observations that will fill each cell. `id_cols` is an optional parameter used to declare the set of columns that uniquely identifies each observation. In practice, it drops all the other columns in the data set. Hence, if we want to keep all the other variables, just skip it.     

```{r ch00_chunk10}

covid_wide_01 <- covid_data %>% 
  tidyr::pivot_wider(id_cols     = c('date', 'location'), 
                     names_from  = 'location', 
                     values_from = 'new_cases')

covid_wide_01

```

Notice that now we have 239 columns rather than 67 of the original data set with each cell conveying the whole information: new cases in a given date for a specific country. So, what if we wanted to have both `new_cases` and `new_deaths` columns in wide form? We just need to provide a vector with the desired variables in `values_from`. By default, the new columns will be named according to the following pattern: **variable_location**.

Since the variables names already contain an underscore, it's a good idea to set a different character as separator. This is because we might need to reverse the operation later for a given task, then it's much easier to identify it. Otherwise, we'd have to use regular expression to inform the specific position of the repeated character -- for example, whether it's the first or the second underscore. 

```{r ch00_chunk11}

covid_wide_02 <- covid_data %>% 
  tidyr::pivot_wider(id_cols     = c('date', 'location'), 
                     names_from  = 'location', 
                     values_from = c('new_cases', 'new_deaths'),
                     names_sep   = '-')

covid_wide_02

```

Our new data set expanded to 477 columns. As we create more and more columns to get a wide data set it might become harder to perform some simple tasks. For example, using `filter` is generally easier than using a conditional `select` when we want to keep only the relevant data. 

In summary, long format may be preferable over wide format when the data set contains more than one grouping variable or we want to work on more than one variable. Besides, long format data sets are ideal for plotting with `ggplot2` package as we'll see later. 

Therefore, it's not unusual to convert a data set from wide to long format. The syntax is very similar to what we saw earlier when converting from long to wide format. The unique difference is in `cols` argument, used to declare what columns we want to stack. However, since wide data sets usually have a large number of columns and we're often interested in putting all of them in long format, it's much easier to declare what columns we want to leave out (`-`). 

```{r ch00_chunk12}

covid_long_01 <- covid_wide_02 %>% 
  tidyr::pivot_longer(cols      = -'date',
                      names_to  = c('variable', 'location'),
                      values_to = 'value',
                      names_sep = '-')

covid_long_01

```

Note that now even the variables (new_cases and new_deaths) are stored in a single column (variable). This is probably an abuse of language, but I call this a complete long format -- as opposed to the original format where the data set was only partially in the long format (the variables were indeed in wide format). For most applications, I think this is the best way to organize the data. 

Converting a data set between wide and long formats might not completely solve our problem. Sometimes, two pieces of information are merged in a single column. For example, suppose that the `location` column had both the continent and country names instead of only the country as in the original data set. We'll call this data set `covid_loc_cont`.

```{r ch00_chunck13, echo = F}

covid_loc_cont <- covid_data %>% 
  dplyr::distinct() %>% 
  dplyr::filter(!is.na(location), !is.na(continent)) %>% 
  tidyr::unite(c('location', 'continent'), col = 'location', sep = '_') %>% 
  dplyr::select(location, date, new_cases, new_deaths)

covid_loc_cont

```

This is undesirable since we can no longer use `group by` or `filter` over continents alone, for instance. Hence, the best practice is to have a single column for each variable. This can be easily achieved using the `separate` function, with a highly self-explanatory syntax. Again, the separator character being unique in the string makes the job much easier.

```{r ch00_chunck14}

covid_separate <- covid_loc_cont %>% 
  dplyr::distinct() %>% 
  tidyr::separate(col  = 'location',
                  into = c('location', 'continent'),
                  sep  = '_')

covid_separate

```

The only caveat to all this simplicity is when we have non-trivial separators. For example, imagine that we had no underscore to separate location from continent.   

```{r ch00_chunck15, echo = F}

covid_loc_cont2 <- covid_data %>% 
  dplyr::distinct() %>% 
   dplyr::filter(!is.na(location), !is.na(continent)) %>% 
  tidyr::unite(c('location', 'continent'), col = 'location', sep = '') %>% 
  dplyr::select(location, date, new_cases, new_deaths)

covid_loc_cont2

```

How could we manage to separate them? Ideally, we should provide a regular expression (or simply regex) to match the appropriate pattern to split the string into location and continent (the `sep` argument works with regex). If you don't know what regex is, think of it as a code used to match patterns, positions and all kinds of features in a string.   

At first glance, a natural choice would be to split the string as of the second uppercase letter. This would work for Afghanistan, France, Netherlands, Chile and all single-worded countries. However, this would fail for countries with two or more words: United States, New Zealand and many others. 

Then, you could argue that a more general approach would be to use regex to match the last uppercase letter in the string. Not actually, because we have a couple of two-worded continents: North America and South America. So, for example, **CanadaNorth America** would be split into **CanadaNorth** and **America** instead of **Canada** and **North America**.  

More often than not, the direct solution is the most difficult to implement (or we simply don't know how to accomplish it) and so we have to think about alternative ways. Data science is all about this. Since a regex solution alone might be very tough or even unfeasible, we'll get back to this problem later when covering text manipulation with the `stringr` package.      

For now, let's just get done with the `tidyr` package looking at two other very commonly used functions. The first one is `unite`, which is the counterpart of `separate`. We can use it to convert the `covid_separate` data frame back to the original form as in `covid_loc_cont`.

```{r ch00_chunk16}

covid_unite <- covid_separate %>% 
  tidyr::unite(c('location', 'continent'), col = 'location', sep = '_')

covid_unite

```

Last but not least is the `nest` function. The `nest` function is usually used in conjunction with `dplyr::group_by` in order to create a new format of data frame in which every cell is now a list rather than a single observation and, thus, might store any kind of object -- data frames, lists, models, plots and so forth. 

The example below shows how to create a nested data frame for the covid data grouped by country. Note that each cell on column `data` is now a data frame itself corresponding to the data for each country.

```{r ch00_chunk17}

covid_eu_nest <- covid_data %>% 
  dplyr::filter(continent == 'Europe') %>% 
  dplyr::group_by(location) %>% 
  tidyr::nest()

covid_eu_nest

```

I find it most useful when we need to perform several tasks over whole datasets. For example, if we had to transform the raw data, create graphics and fit models for each country in the covid dataset. The base R solution in this case would be to create list of lists, which can be very confusing sometimes. We'll come back to nested data frames when we talk about functional programming with the `purrr` package.

### Text manipulation

The need to handle text data has grown substantially in parallel with the popularity of Machine Learning models and, more specifically, Natural Language Processing (NLP). For those more advanced applications, the challenge is to standardize a large set of (messy) texts in order to extract features which can then feed the models and generate predictions. 

Nevertheless, knowing the basics of text manipulation is critical for everyday tasks. It includes subsetting parts of a word, detecting if specific patterns are present, replacing a sequence of characters by something else and so forth. The functions from the `stringr` package do a terrific job in simplifying all these operations and go far beyond. 

Similarly to what we did in the previous sections, we'll focus on the most widely used functions. Let's start with `str_detect`, which is conveniently used in conjunction with `dplyr::filter` since it returns `TRUE` if the specific pattern is found in the string or `FALSE` otherwise.

For example, let's say we want to analyze the covid data only for North and South Americas. We've learned how to do so using `dplyr::filter`.

```{r ch00_chunk18}

covid_americas <- covid_data %>% 
  dplyr::filter(continent %in% c('North America', 'South America')) 

```

This one is not cumbersome. But let's pretend there are, say, 50 continents on Earth with 25 having 'America' in their names. Would it still make sense to write all these 25 names in a vector? Absolutely not. Since all of them share a commom pattern, we can easily employ `str_detect` to do the trick:

```{r ch00_chunk19}

covid_americas2 <- covid_data %>% 
  dplyr::filter(stringr::str_detect(continent, 'America'))

```

Note that we could have left aside several characters using only, say, 'Am' or 'Ame'. This would have worked fine if there was no other continent with this sequence of characters. Of course, this parsimony makes more sense for lengthy words or long sentences. For short words it's recommended to write the full word in order to prevent any undesirable output. 

In addition, we can also provide multiple patterns to `str_detect` by separating them with a `|`. 

```{r ch00_chunk20}

covid_americas3 <- covid_data %>% 
  dplyr::filter(stringr::str_detect(continent, 'South|North'))

```

Finally, we may use the `negate = TRUE` argument if we're interested in the opposite of the pattern we provide -- pretty much like `!` in conditions. It's specially useful when we want to keep most categories but one (or a few). For example, suppose that now we want to analyze every continent except for Asia. Instead of writing them all, we could simply do the following:

```{r ch00_chunk21}

covid_exAsia <- covid_data %>% 
  dplyr::filter(stringr::str_detect(continent, 'Asia', negate = TRUE))

```

Another recurrent task when we're dealing with strings is to remove a part of it. It's generally required in order to establish a standard within categories so we can perform further operations. The complexity of this task varies depending on the form of this undesired part. 

The most simple case is when we have to remove a sequence of characters which is fixed both in length and in position. For example, suppose we have a data frame `covid_numCont` in which the observations in the continent column starts with a random number from 0 to 9 -- think of it as a typing error from the source, since the same reasoning applies if those random numbers were present only in a few observations.

```{r ch00_chunk22, echo = F}

set.seed(123)

covid_numCont <- covid_data %>% 
  dplyr::filter(!is.na(continent)) %>% 
  dplyr::mutate(continent = sapply(continent, function(x) paste(sample(1:9, 1), x, sep = '.'))) %>% 
  dplyr::select(2:4, 6)


covid_numCont %>% 
  dplyr::slice_sample(n =  10)

```

To solve this is solely a matter of subsetting the string from position three onwards using `str_sub`. The `end` argument defaults to last character, so we don't need to explicit it. 

```{r ch00_chunk23}

covid_numCont %>% 
  dplyr::mutate(continent = stringr::str_sub(continent, start = 3))

```

Nice. But what if the random numbers ranged from 0 to 10? 

```{r ch00_chunk24, echo = F}

set.seed(123)

covid_numCont <- covid_data %>% 
  dplyr::filter(!is.na(continent)) %>% 
  dplyr::mutate(continent = sapply(continent, function(x) paste(sample(1:10, 1, prob = c(rep(0.3, 9), 0.6)), x, sep = '.'))) %>% 
  dplyr::select(2:4, 6)


covid_numCont %>% 
  dplyr::slice_sample(n =  10)

```

With an extra digit, we could no longer resort to the previous solution. We won't dedicate an exclusive section to regular expressions, but simple examples will eventually show up throughout the book. In this case, we could use a simple regular expression inside `str_remove` to get rid of everything before and up to the `.`, `.*\\.`. 

```{r ch00_chunk25}

covid_numCont %>% 
  dplyr::mutate(continent = stringr::str_remove(continent, ".*\\."))

```

Typing errors may arise under different forms along the character column. These cases usually require a more thorough evaluation and, more often than not, the solution is to manually replace the wrong words by the correct ones. For instance, in the data frame below (named `covid_typo`) we can find two different typos for North America: there's a missing **h** in rows 1 and 5; while there's an extra **h** in rows 3 and 8. 

```{r ch00_chunk26, echo = F}

set.seed(123)

covid_typo <- covid_data %>% 
  dplyr::filter(continent == 'North America') %>% 
  dplyr::select(2:4, 6)

covid_typo[c(1,5), ]$continent <- 'Nort America'
covid_typo[c(3,8), ]$continent <- 'Northh America'

covid_typo

```

Since the data frame contains a huge number of observations, it may be present in other positions as well. We can use `str_replace` to fix it for the whole column. 

```{r ch00_chunk28, eval = FALSE}

covid_typo %>% 
  dplyr::mutate(
    continent = stringr::str_replace(continent, 'Nort America', 'North America'),
    continent = stringr::str_replace(continent, 'Northh America', 'North America')
    )

```

Or we can simply pass a vector with all the replacements to the `str_replace_all` function.

```{r ch00_chunk29}

covid_typo %>% 
  dplyr::mutate(
    continent = stringr::str_replace_all(
      continent, 
      c('Nort America'   = 'North America',
        'Northh America' = 'North America')
    )
  )

```

There's one last kind of typo we can't help but to mention: whitespace. Whitespaces are particularly troublesome when they're misplaced in the start/end of string or repeated inside it. Because they're very easy to go unnoticed the `stringr` package contains two functions to cope with them: `str_trim` and `str_squish`. The former removes whitespaces from start/end of the string, whereas the later removes them from inside of a string. 

The data frame below (named `covid_ws`) uses the same example as above, but now with a whitespace in the end of observations 1 and 5; and a repeated whitespace inside the observations 3 and 8. Note that `tibble` automatically adds quotation marks around the strings to highlight the extra whitespaces. This is awesome!

```{r ch00_chunk30, echo = F}

set.seed(123)

covid_ws <- covid_data %>% 
  dplyr::filter(continent == 'North America') %>% 
  dplyr::select(2:4, 6)

covid_ws[c(1,5), ]$continent <- 'North America '
covid_ws[c(3,8), ]$continent <- 'North  America'

covid_ws

```

We can easily get rid of the whitespaces using those functions. It's advisable to use them as a preprocessing step whenever we're working with character columns that should not have these extra whitespaces.

```{r ch00_chunk32, eval = FALSE}

covid_ws %>% 
  dplyr::mutate(
    continent = stringr::str_trim(continent),
    continent = stringr::str_squish(continent)
  )

```

```{r ch00_chunk31, echo = F}

set.seed(123)

covid_ws <- covid_data %>% 
  dplyr::filter(continent == 'North America') %>% 
  dplyr::select(2:4, 6)

covid_ws[c(1,5), ]$continent <- 'North America ' 
covid_ws[c(3,8), ]$continent <- 'North  America'

covid_ws %>% 
  dplyr::mutate(
    continent = stringr::str_trim(continent),
    continent = stringr::str_squish(continent)
  )

```

To finish up, let's use the tools we've just learned to solve that problem in subsection \@ref(datalay). To recap, we wanted to separate the column `location` into country and continent. The issue was that with no separator character between the two names, we should resort to any kind of complicated regular expression to do the trick. 

```{r, ref.label=I('ch00_chunck15'), echo = F}

```

As I said earlier, data science is all about finding workarounds. Of course we're often interested in general approaches, but sometimes we have to settle for a lower level solution which gets the job done. For example, in this case we could waste a long time figuring out the best possible solution whereas a simpler one is at hand. 

Remember that we can use `stringr::replace_all` to replace a set of patterns in a string. In addition, we could easily employ `tidyr::separate` if we had a unique separator character. Therefore, all we have to do is to add a separator character right before continent names. Since we have only six continents it won't be cumbersome. We'll call this tibble `covid_loc_cont2`:

```{r, ch00_chunk33}

covid_loc_cont2 %>% 
  dplyr::mutate(location = stringr::str_replace_all(
    location,
    c('Asia'          = '_Asia',
      'Europe'        = '_Europe',
      'Africa'        = '_Africa',
      'South America' = '_South America',
      'North America' = '_North America',
      'Oceania'       = '_Oceania'))) %>% 
  tidyr::separate(location, 
                  c('country', 'continent'),
                  '_')

```

Job done! Next we turn to dates, which are a special form of string. Handling them properly is essential to analyze time series data.   

### Date manipulation

Having knowledge on date manipulation is crucial to perform a lot of tasks when we're dealing with time series data. Date objects are very convenient since they allow us to extract features that can be used for many purposes. The subject is so vast that tidyverse has developed the `lubridate` package exclusively to handle date objects.

The first step when we're working with dates is to convert the string to a date object. In order to get rid of ambiguity issues, `lubridate` contains a set of predefined functions that take into account the ordering of year, month and day in the string. 

For instance, if we have a date in the standard `YYYY-MM-DD` format we can use the `ymd` function. Note that it works regardless of how these terms are separated: it might be like `2021-12-01`, `2022/12/01` or even `20221201`. Also, months names (full or abbreviated forms) are allowed: `2021-December-01` or `2021-Dec-01`. The same logic applies to the whole family of related functions: `mdy`, `dmy`, `ydm`, `dym`, `my`, `ym` and so on. 

```{r, ch00_chunk332}

lubridate::ymd('2022/12/01')
lubridate::mdy('december, 1, 2022')
lubridate::dmy('01122022')
lubridate::my('Dec-2021')

```

In case the string format doesn't match any of the predefined patterns, we can use `lubridate::as_date` function and declare the unusual format using specific operators: `%Y` for year; `%m` for month; and `%d` for day. There are two other useful ones: `%b` for months names (full or abbreviated) and `%y` for two-digits year. 

Now, let's see how to extract features from date objects and how to use them to perform common operations. Take the following data set (named `brl_usd`) which provides daily values of the Brazilian Real (BRL) versus the US Dollar from January 2010 to December 2021 where the column `date` is in the `DD/MM/YYYY` format. 

```{r, ch00_chunk34, echo = FALSE}

brl_usd <- rbcb::get_series(list('brl' = 1), start_date = '2010-01-04', end_date = '2021-12-31') %>% 
  dplyr::mutate(date = glue::glue('{lubridate::day(date)}/{lubridate::month(date)}/{lubridate::year(date)}') %>% as.character())

brl_usd

```

Note that the column date is in standard string (or character) format. We'll first convert it to the appropriate date format using the functions we've just learned.

```{r, ch00_chunk35}

brl_usd_aux <- brl_usd %>% 
  dplyr::mutate(date = lubridate::dmy(date))

brl_usd_aux

```

Now, suppose we want to obtain BRL monthly average. We have two ways to do this, more logical or more compact (see later when we talk about rounding dates). In the logic way, all we have to do is to create the columns we need to perform a grouping operation: year and month. 

```{r, ch00_chunk36}

brl_usd_monthly <- brl_usd_aux %>% 
  dplyr::mutate(year  = lubridate::year(date),
                month = lubridate::month(date)) %>% 
  dplyr::group_by(year, month) %>% 
  dplyr::summarise(brl_monthly = mean(brl))

brl_usd_monthly

```

You might be wondering how to recover the date format `YYYY-MM-DD`. We can do it by simply using `lubridate::make_date`. This function creates a standard date object from user-provided year, month and day. Since we've aggregated daily into monthly, we have two common choices for the day parameter: we either set it to 1 (the default) or we set it to the last day of the month using `lubridate::days_in_month`.

```{r, ch00_chunk36.2}

brl_usd_monthly %>% 
  dplyr::mutate(date  = lubridate::make_date(year  = year, 
                                            month = month, 
                                            day   = 1),
                date2 = lubridate::make_date(year = year, 
                                             month = month,
                                             day   = lubridate::days_in_month(date)))

```

Note that creating a column with the number of days in each month may be itself particularly useful. For instance, when we have only the monthly totals of a variable and we need to compute daily averages. And, of course, it works fine with February since it takes the year into account. 

The same procedure applies if we want to get quarterly means. It's just a matter of creating a column with quarters. Be aware, however, that the `quarter` function has a parameter named `with_year` that when is set to `TRUE` eliminates the need to create a separate column for the year. 

```{r, ch00_chunk37}

brl_usd_quarterly <- brl_usd_aux %>% 
  dplyr::mutate(quarter = lubridate::quarter(date, with_year = TRUE)) %>% 
  dplyr::group_by(quarter) %>% 
  dplyr::summarise(brl_quarterly = mean(brl))

brl_usd_quarterly

```

Special attention must be taken when we want to work with weeks, because `lubridate` has two different functions to extract this feature: `week` and `isoweek`. The former returns the number of complete seven day periods that have occurred between the date and January 1st, while the latter returns the number of the week (from Monday to Sunday) the date belongs to. 

To get a better sense of the difference between them, suppose we provide the date '2022-01-01'. `lubridate::week` will return 1, since it's in the first seven days period after January 1st. On the other hand, `lubridate::isoweek` will return 52 because it's Saturday and thus part of the last week of the previous year. It'll only return 1 as of '2022-01-03' since it belongs to a new week.

```{r, ch00_chunk38}

lubridate::week('2022-01-01')
lubridate::isoweek('2022-01-01')
lubridate::isoweek('2022-01-03')

```

Therefore, if we want to compute weekly averages and by weekly we mean a period of seven days in a row then we should pick `isoweek` instead of `week`. Another feature we can extract from dates is the week day. In addition to temporal aggregation, it's often used to filter or label data we want to plot later.

```{r, ch00_chunk39}

brl_usd_aux %>% 
  dplyr::mutate(wday = lubridate::wday(date, label = TRUE))

```

Now we turn to operations with date objects. The `lubridate` package contains two special operators `%m+%` and `%m-%` that work nicely with date objects to perform, respectively, addition and subtraction.

```{r, ch00_chunk40}

library(lubridate)

d1 <- lubridate::ymd('2020-02-29')

d1 %m+% years(2)
d1 %m-% months(3)
d1 %m+% days(1)

```

In addition, there's also the lesser known `add_with_rollback` function which we can use to have more control of the output. For example, when adding one month to 2022-01-31 we might want either 2022-02-28 (the last day of the next month) or 2022-03-01 (a period of one month). To get the latter, we set the `roll_to_first` parameter to `TRUE`.

```{r, ch00_chunk41}

d2 <- lubridate::ymd('2022-01-31')

lubridate::add_with_rollback(d2, months(1), roll_to_first = TRUE)
lubridate::add_with_rollback(d2, months(1), roll_to_first = FALSE)

```

I couldn't help but mentioning two useful functions used to round dates: `floor_date` and `ceiling_date`. They take a date object and rounds it down or up, respectively, to the nearest boundary of the specified time unit.

```{r, ch00_chunk42}

d3 <- lubridate::ymd('2021-03-13')

lubridate::floor_date(d3, unit = 'month')
lubridate::ceiling_date(d3, unit = 'month')
lubridate::floor_date(d3, unit = 'quarter')
lubridate::ceiling_date(d3, unit = 'quarter')

```

These functions can be helpful in several ways and you'll find their benefits as you go through your own tasks. For example, if we want to match (or re-write) dates that refer to the same period but are written differently (monthly date in data set A is '2021-12-01' and in data set B is '2021-12-31'). 

I use them very often as a simpler way to perform temporal aggregation. Remember that earlier in this section we computed monthly averages by creating two grouping columns, year and month. The logic was simply to treat every day in the same year/month as belonging to the same group. We can easily accomplish the same result by rounding dates down, with the great benefit of preserving the date column. 

```{r, ch00_chunk43}

brl_usd_monthly2 <- brl_usd_aux %>% 
  dplyr::mutate(date = lubridate::floor_date(date, unit = 'month')) %>% 
  dplyr::group_by(date) %>% 
  dplyr::summarise(brl_monthly = mean(brl))
  
brl_usd_monthly2

```

To finish up, let's have a quick look at a family of functions: `wday`, `mday`, `qday` and `yday`. They're used to get the number of days that have occurred within that time period, respectively. 

```{r, ch00_chunk44}

lubridate::wday('2021-06-10') # 5th day of that week
lubridate::qday('2021-06-10') # 71th day of the 2nd quarter of 2021
lubridate::yday('2021-06-10') # 161th day of 2021

```

It's very useful, for example, when you need to compare observations from the same period in different years or create high frequency seasonal variables.

## Looping

Iteration is an indispensable tool in programming and every language has its own structure. Essentially, loops are used to repeat an action over a set of values, thus preventing us from the annoying and risky copying-and-pasting thing. Whenever we have any kind of redundancy, there's a good reason to use loops.

The `purrr` package provides many interesting tools for working with functions and vectors. For our purposes, we'll stick with the family of `map` functions. The logic will be always the same: applying a function -- existing or user-defined -- over a vector (or list) of arguments. 

Let's start with a very simple example. Suppose we have three numeric vectors and we want to compute their means. Instead of calling `mean` over each vector separately, we can put them into a list and then use the `map` function in conjunction with the existing `mean` function.

```{r, ch00_chunk45}

v1 <- c(1,4,7,8)
v2 <- c(3,5,9,0) 
v3 <- c(12,0,7,1)

v_all <- list(v1, v2, v3)

purrr::map(.x = v_all, .f = mean)

```

Note that by default the output will be a list, but we can have other output formats using `map_*`: `map_dbl` will return the results as a vector, whereas `map_dfc` will return them in a (column) data frame. We only need to consider whether or not the output can be coerced to the desired class/format. For example, we would get an error if we tried `map_lgl` since it's not possible to coerce a numeric object into logical.  

```{r, ch00_chunk46}

purrr::map_dbl(.x = v_all, .f = mean)
purrr::map_dfc(.x = v_all, .f = mean)

```

Most of the time I prefer to return the results as a list, because that makes it easier to apply further operations if needed. Data frames are usually a better choice for final results.  

Now, let's introduce some degree of complexity to the exercise by providing our own function. For this, let's use the example of importing data from an API we saw earlier in subsection \@ref(import). Suppose that in addition to CPI we also want to get the time series for GDP and Unemployment rate. 

Remember (or scroll up if necessary) that we created an object called `api_series_id` with the ID of the CPI time series and that was the only specific parameter -- everything else would be the same for any other series we wanted. Therefore, our first task here is to create a function whose only parameter is the series ID. Then, we create a vector (or list) with the desired series ID's and -- guess what? -- use them inside the `map` function.  

I'll create the `get_series` function using the same content we already saw up there, but leaving the `api_series_id` as a parameter (series_id). Note that I'll keep some objects with their original names -- starting with *cpi_* -- just to avoid confusion. This has no practical effect, though.     

```{r, ch00_chunk47}

get_series <- function(series_id){
  api_url       <- 'https://api.stlouisfed.org/fred/series/observations?'
  api_key       <- 'bc0b91e1c6fa2cab4d4f180c7965edfa'
  api_series_id <- series_id
  obs_start     <- '2010-01-01'
  api_filetype  <- 'json'
  api_request   <- glue::glue('{api_url}series_id={api_series_id}&observation_start={obs_start}&api_key={api_key}&file_type={api_filetype}')
  cpi_request   <- httr::GET(url = api_request)
  cpi_content   <- httr::content(cpi_request, as = 'text')
  cpi_list      <- jsonlite::fromJSON(cpi_content, flatten = FALSE)
  cpi_tbl       <- cpi_list[['observations']] %>% tibble::as_tibble()
  return(cpi_tbl)
} 

```

We can test our function using the CPI's ID we used before. 

```{r, ch00_chunk48}

get_series(series_id = 'CPALTT01USM657N')

```

Great, it's working fine! The next step is to create a vector (or list) with the ID of each series. Assigning names to the vector (list) elements is a good idea since these names are carried forward helping to identify the elements in the output list. 

```{r, ch00_chunk49}

id_list <- list('CPI'   = 'CPALTT01USM657N',
                'GDP'   = 'GDPC1',
                'Unemp' = 'UNRATE')

fred_data <- purrr::map(.x = id_list, .f = get_series)
fred_data

```

We could make our function more general by allowing more parameters to vary. For example, we could have a different time span for each series (the `obs_start` object). The procedure would be almost the same: we would add an extra parameter in the function, create two vectors (lists) with the parameters values and use `map2` instead of `map`. 

```{r, ch00_chunk50}

get_series2 <- function(series_id, series_start){
  api_url       <- 'https://api.stlouisfed.org/fred/series/observations?'
  api_key       <- 'bc0b91e1c6fa2cab4d4f180c7965edfa'
  api_series_id <- series_id
  obs_start     <- series_start
  api_filetype  <- 'json'
  api_request   <- glue::glue('{api_url}series_id={api_series_id}&observation_start={obs_start}&api_key={api_key}&file_type={api_filetype}')
  cpi_request   <- httr::GET(url = api_request)
  cpi_content   <- httr::content(cpi_request, as = 'text')
  cpi_list      <- jsonlite::fromJSON(cpi_content, flatten = FALSE)
  cpi_tbl       <- cpi_list[['observations']] %>% tibble::as_tibble()
  return(cpi_tbl)
} 

id_list <- list('CPI'   = 'CPALTT01USM657N',
                'GDP'   = 'GDPC1',
                'Unemp' = 'UNRATE')

time_list <- list('CPI'   = '2010-01-01',
                  'GDP'   = '2012-04-01',
                  'Unemp' = '2014-06-01')  

fred_data2 <- purrr::map2(.x = id_list, .y = time_list, .f = get_series2)

```

We must be careful with the example above because it may give the impression that `map2` takes into account the variables names in the process. It doesn't! It actually considers how elements are sorted in each list. So if we changed the CPI to the second position in `time_list`, then we would end up with GDP data with CPI time span.

A safer, thus preferable, alternative is to use names rather than positions as indexes. Since we can use `list[['element_name']]` to access an element in a list, we may reduce our problem to a single dimension by looping over variables names which are the same in both lists.  

```{r, ch00_chunk51}

vars_fred <- c('CPI', 'GDP', 'Unemp')
fred_data2_names <- purrr::map(
  .x = vars_fred, 
  .f = function(x) get_series2(series_id = id_list[[x]],
                               series_start = time_list[[x]]
  )
) %>% 
  magrittr::set_names(vars_fred)

```

Notice how powerful this solution is: you can generalize it to as many parameters as you need without taking the risk of using the parameter value of one series into another, with the additional work being only to make explicit the parameters of the function in `.f`. 

To finish this topic, let's get back to the end of subsection \@ref(datalay). There, we had an overview of nested data frames and I stated that I find them most useful when we need to perform several tasks over whole data sets. Also, remember that nesting a data frame is, roughly speaking, converting every cell from a single observation into a list. 

Let's print again the nested data frame we created up there, named `covid_eu_nest`.

```{r, ch00_chunk52, echo = FALSE}

covid_eu_nest

```

We can use `map` to perform computations for every country at once. Moreover, we can create new columns to store the results, thus gathering all the information in the same object.

```{r, ch00_chunk54}

covid_eu_nest %>% 
  dplyr::mutate(
    max_total_cases = purrr::map_dbl(.x = data, 
                                 .f = function(x){
                                   x %>% 
                                     dplyr::pull(total_cases) %>% 
                                     max(na.rm = TRUE)}),
    min_total_cases = purrr::map_dbl(.x = data, 
                                 .f = function(x){
                                   x %>% 
                                     dplyr::pull(total_cases) %>% 
                                     min(na.rm = TRUE)}))

```

And since each cell is a list rather than an observation, we're by no means restricted to numeric elements. We could use the same strategy to create a column with plots, for example. Plotting is, by the way, the subject of the next section.

## Plotting

We're almost there in our goal to review the basics of Tidyverse. Succinctly describing all the previous sections was challenging, but I believe you had enough information to start carrying out data analyzes on your own. When it comes to making graphics, extrapolating from the basics is somewhat harder because there are infinite possibilities for customization. 

In fact, the grammar used in `ggplot2` package is broad and far from simple at first glance. But as you practice, it becomes increasingly graspable. Still, you'll often need to turn to external resources to learn how to insert specific features -- Stack Overflow is of great help on this.

Let's think of the process of making a graphic as a set of layers arranged sequentially. The first layer is the data you want to plot. We'll use the CPI data from subsection \@ref(readapi). The second layer is the `ggplot` function. So far, there's nothing to visualize. All these two layers do is to set the stage for what's coming next.  

```{r, ch00_chunk55, eval = F}

cpi_tbl %>% 
  ggplot()

```

The third layer we must provide is the `geom`, which is the geometry used to represent our data. This is the most important layer as it contains a set of parameters that effectively creates a visualization of the data. For time series data we generally use a line graph, so `geom_line` is the appropriate geometry. In addition, we need to provide the values for **x** and **y** axes. In this case, they're the dates and CPI values, respectively.

Before proceeding, we need to make sure that the values for **x** and **y** are in the appropriate format (or class). We can use the `glimpse` function from `dplyr` package to check this out.

```{r, ch00_chunk56}
library(ggplot2)
cpi_tbl %>%
  dplyr::glimpse()

```

We see that both `date` and `value` (the CPI value) are in character format and so we need to convert them to date and numeric format, respectively. This is easily achieved by the `mutate` function from `dplyr` package we've learned in subsection \@ref(wrangling-data-manipulation).

One last thing we need to be aware of is that `ggplot` uses `+` instead of `%>%` as an operator to chain actions. Therefore, every layer we add is preceded by a `+`. The code below produces the simplest plot from CPI data.

```{r, ch00_chunk57}

cpi_tbl %>%
  dplyr::mutate(date = lubridate::ymd(date),
                value = as.numeric(value)) %>% 
  ggplot() + 
  geom_line(aes(x = date, y = value))

```

What should we care about this graphic? Well, every graphic must contain a meaningful title and labels for both x and y axes. We can set them using the `labs` layer. Additionally, we might want to have shorter intervals for dates (x-axis) and values (y-axis). There are also specific layers to control axes settings. 

```{r, ch00_chunk58}

cpi_tbl %>%
  dplyr::mutate(date = lubridate::ymd(date),
                value = as.numeric(value)) %>% 
  ggplot() + 
  geom_line(aes(x = date, y = value)) +
  labs(title = 'US CPI showing an upward trend as of 2021',
       x     = 'Date',
       y     = 'US Monthly CPI (%)') +
  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +
  scale_y_continuous(breaks = seq(-1, 1, 0.25))

```

Finally, we could be interested in adding some feature to the plot in order to highlight the upward trend as of 2021. Either the 12-month-accumulated CPI or the 3-months-accumulated seasonally-adjusted CPI would be common choices for the task and we'll see examples of how to perform this kind of computation later. For now, just to keep things simple let's use the average CPI from 2010 to 2019 (the pre-COVID period) as a measure of 'normal' CPI. 

```{r, ch00_chunk59}

cpi_tbl %>%
  dplyr::mutate(date = lubridate::ymd(date),
                value = as.numeric(value)) %>% 
  dplyr::mutate(value_avg = mean(value[which(lubridate::year(date) %in% 2010:2019)])) %>% 
  ggplot() + 
  geom_line(aes(x = date, y = value)) +
  geom_line(aes(x = date, y = value_avg), color = 'red', lwd = 1) +
  labs(title    = 'US CPI showing an upward trend as of 2021',
       subtitle = 'Red line is 2010-2019 average',
       x        = 'Date',
       y        = 'US Monthly CPI (%)') +
  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +
  scale_y_continuous(breaks = seq(-1, 1, 0.25))

```

Since the parameter `x = date` appears in both `geom_line` layers, we could make the code more compact by moving this parameter to `ggplot()` function. All the parameters inside `ggplot()` are carried forward to every subsequent layer. It saves a lot of effort when we're adding geoms to a graphic with fixed features. In this graphic, `x` will always be the date variable regardless of any layer I insert on it.  

```{r, ch00_chunk60, eval = F}

cpi_tbl %>%
  dplyr::mutate(date = lubridate::ymd(date),
                value = as.numeric(value)) %>% 
  dplyr::mutate(value_avg = mean(value[which(lubridate::year(date) %in% 2010:2019)])) %>% 
  ggplot(aes(x = date)) + 
  geom_line(aes(y = value)) +
  geom_line(aes(y = value_avg), color = 'red', lwd = 1) +
  labs(title    = 'US CPI showing an upward trend as of 2021',
       subtitle = 'Red line is 2010-2019 average',
       x        = 'Date',
       y        = 'US Monthly CPI (%)') +
  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +
  scale_y_continuous(breaks = seq(-1, 1, 0.25))

```

You should be asking yourself why `x` and `y` parameters went inside the `aes` function whereas `color` went outside it. First of all, the parameters which set the graphic axes always go inside `aes`. Other parameters have a special role whether they appear inside or outside `aes`. According to the documentation, *aesthetic mappings describe how variables in the data are mapped to visual properties (aesthetics) of geoms*. It's much more easier to understand through an example. 

In the graphic above, we used `color` outside `aes` to define which color we wanted for the graphic line. However, if we were to use `color` as an attribute to highlight different groups from the data, `color` should go inside `aes`. In addition, we should pass a variable onto it instead of color names. This variable might be discrete, in which case we would have one color per group; or it might be continuous, in which case we would have a color gradient.

In the piece of code below we create a new variable named *covid_period* to separate the CPI data between pre-Covid and Covid periods and use it as a color attribute.

```{r, ch00_chunk61}
library(tidyverse)
cpi_tbl %>%
  dplyr::mutate(date = lubridate::ymd(date),
                value = as.numeric(value),
                covid = if_else(date < '2020-03-01', 'No', 'Yes')) %>% 
  ggplot() + 
  geom_line(aes(x = date, y = value, color = covid), lwd = 1) +
  labs(title    = 'US CPI showing an upward trend as of 2021',
       x        = 'Date',
       y        = 'US Monthly CPI (%)') +
  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +
  scale_y_continuous(breaks = seq(-1, 1, 0.25))

```

Note that `ggplot` automatically assigns colors for the attributes and adds a legend. So we could be interested in customize both the attributes' colors and legend position. Again, this might be achieved by adding two specific layers. The logic will always be the same: **parameters inside `aes` turn a variable into an attribute which might then be customized by specific layers**. 

```{r, ch00_chunk62}

cpi_tbl %>%
  dplyr::mutate(date = lubridate::ymd(date),
                value = as.numeric(value),
                covid = if_else(date < '2020-03-01', 'No', 'Yes')) %>% 
  ggplot() + 
  geom_line(aes(x = date, y = value, color = covid), lwd = 1) +
  labs(title    = 'US CPI showing an upward trend as of 2021',
       x        = 'Date',
       y        = 'US Monthly CPI (%)') +
  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +
  scale_y_continuous(breaks = seq(-1, 1, 0.25)) +
  scale_color_manual(values = c('darkgreen', 'orange')) +
  theme(legend.position = 'top')
  
```

As a final example, we'll see how to build a scatter plot and explore some additional features from `ggplot`. Scatter plots are used to highlight the relationship between two variables in a given point in time. For this task, let's take the Covid data set from subsection \@ref(import). We'll first filter the data to a given point in time, say '2021-07-31'. Next, we'll plot **people_fully_vaccinated_per_hundred** and **new_deaths_smoothed_per_million** on the **x** and **y** axes, respectively. 

```{r, ch00_chunk63}

covid_data_from_local %>% 
  dplyr::filter(date == '2021-07-31') %>% 
  ggplot() +
  geom_point(aes(x = people_fully_vaccinated_per_hundred,
                 y = new_deaths_smoothed_per_million))
  
```

Be aware that we're not trying to establish a causal relationship, nor to find a threshold for vaccination above which people are safe. We must be careful when interpreting this graphic, since omitted variables such as age and previous exposure to the virus may influence the outcome. 

A possible message we can extract from it is: *new deaths in late July were above 5 per million people in countries where vaccination falls below 30% of population*. So let's put this in the graph title. In addition, let's use colors to represent each continent (inside `aes` since it's a mapping attribute) and increase the size of the points (outside `aes` since it's not a mapping attribute).

We saw earlier how to manually set the color to each group. It's worth emphasizing that we're by no means bounded to using colors names as `ggplot` also works with other tools such as HEX codes and the awesome ColorBrewer. ColorBrewer is an amazing source for color schemes carefully designed to improve visual communication. You can check this out at https://colorbrewer2.org/. 

We'll stick to ColorBrewer since it makes it effortless to assign a different color to each of the six continents. Notice that after choosing your desired pattern in the website, you get some meta data (type and scheme) embedded in the link displayed on your browser that will be used here.

In addition, we'll use `theme_light()` to make our plot cleaner. Basically, a theme is a set of pre-defined features for a graph. You can check other built-in themes starting with `theme_`, including `theme_dark()`, `theme_minimal()`, `theme_classic()` and so on.  

```{r, ch00_chunk64}

covid_data_from_local %>% 
  dplyr::filter(date == '2021-07-31',
                !is.na(continent)) %>% 
  ggplot() +
  geom_point(aes(x = people_fully_vaccinated_per_hundred,
                 y = new_deaths_smoothed_per_million,
                 color = continent),
             size = 3) +
  labs(title = 'New deaths in late July were above 5 per million people in countries where vaccination falls below 30% of population') +
  scale_color_brewer(type = 'diverging', palette = 'BrBG') +
  theme_light()
  
```

Falar de facets


## Extra: missing values

Uma seÃ§Ã£o dedicada a missing values: como visualizar e preencher?